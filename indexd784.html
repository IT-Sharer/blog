<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>IT-Sharer</title>
	<atom:link href="http://wp.hhy/?feed=rss2" rel="self" type="application/rss+xml" />
	<link>http://wp.hhy</link>
	<description>温 润 醇 和    臻 于 至 善</description>
	<lastBuildDate>Sat, 04 Oct 2014 05:01:37 +0000</lastBuildDate>
	<language>zh-CN</language>
		<sy:updatePeriod>hourly</sy:updatePeriod>
		<sy:updateFrequency>1</sy:updateFrequency>
	<generator>http://wordpress.org/?v=4.0</generator>
	<item>
		<title>数据挖掘十大算法</title>
		<link>http://wp.hhy/?p=17</link>
		<comments>http://wp.hhy/?p=17#comments</comments>
		<pubDate>Sat, 04 Oct 2014 05:01:37 +0000</pubDate>
		<dc:creator><![CDATA[huahuiyou]]></dc:creator>
				<category><![CDATA[数据挖掘]]></category>

		<guid isPermaLink="false">http://wp.hhy/?p=17</guid>
		<description><![CDATA[国际权威的学术组织the IEEE International Conferen<span class="ellipsis">&#8230;</span><div class="read-more"><a href="http://wp.hhy/?p=17">查阅全文 &#8250;</a></div><!-- end of .read-more -->]]></description>
				<content:encoded><![CDATA[<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:宋体">国际权威的学术组织</span><span style="font-family:Arial">the IEEE International Conference on Data Mining (ICDM) 2006</span><span style="font-family:宋体">年</span><span style="font-family:Arial">12</span><span style="font-family:宋体">月评选出了数据挖掘领域的十大经典算法：</span><span style="font-family:Arial">C4.5, k-Means, SVM, Apriori, EM, PageRank, AdaBoost, kNN, Naive Bayes, and CART.<br />
</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:宋体">不仅仅是选中的十大算法，其实参加<span style="color:black">评选的</span></span><span style="font-family:Arial">18</span><span style="color:black"><span style="font-family:宋体">种算法，实际上随便拿出一种来都可以称得上是经典算法，它们在数据挖掘领域都产生了极为深远的影响。</span><span style="color:#333333; font-family:Arial"> <br />
</span></span></span></p>
<p style="background: white"><span style="color:black; font-family:Arial; font-size:10pt">1. <a href="http://blog.csdn.net/aladdina/archive/2009/04/30/4141048.aspx" target="_blank"><span style="color:#336699">C4.5</span></a><span style="color:#333333"><br />
			</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">C4.5</span><span style="font-family:宋体">算法是机器学习算法中的一种分类决策树算法</span><span style="font-family:Arial">,</span><span style="font-family:宋体">其核心算法是</span><span style="font-family:Arial">ID3</span><span style="font-family:宋体">算法</span><span style="font-family:Arial">.  C4.5</span><span style="font-family:宋体">算法继承了</span><span style="font-family:Arial">ID3</span><span style="font-family:宋体">算法的优点，并在以下几方面对</span><span style="font-family:Arial">ID3</span><span style="font-family:宋体">算法进行了改进：</span><span style="font-family:Arial"><br />
			</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">1) </span><span style="font-family:宋体">用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足；</span><span style="font-family:Arial"><br/>    2) </span><span style="font-family:宋体">在树构造过程中进行剪枝；</span><span style="font-family:Arial"><br/>    3) </span><span style="font-family:宋体">能够完成对连续属性的离散化处理；</span><span style="font-family:Arial"><br/>    4) </span><span style="font-family:宋体">能够对不完整数据进行处理。</span><span style="font-family:Arial"><br />
			</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">C4.5</span><span style="font-family:宋体">算法有如下优点：产生的分类规则易于理解，准确率较高。其缺点是：在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。</span><span style="font-family:Arial"> <br />
</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">2. <a href="http://blog.csdn.net/aladdina/archive/2009/04/30/4141089.aspx" target="_blank"><span style="color:#336699">The k-means algorithm</span></a> </span><span style="font-family:宋体">即</span><span style="font-family:Arial">K-Means</span><span style="font-family:宋体">算法</span><span style="font-family:Arial"><br />
			</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">k-means algorithm</span><span style="font-family:宋体">算法是一个聚类算法，把</span><span style="font-family:Arial">n</span><span style="font-family:宋体">的对象根据他们的属性分为</span><span style="font-family:Arial">k</span><span style="font-family:宋体">个分割，</span><span style="font-family:Arial">k &lt; n</span><span style="font-family:宋体">。它与处理混合正态分布的最大期望算法很相似，因为他们都试图找到数据中自然聚类的中心。它假设对象属性来自于空间向量，并且目标是使各个群组内部的均</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">方误差总和最小。</span><span style="font-family:Arial"> <br />
</span></span></p>
<p style="background: white"><span style="color:#333333; font-family:Arial; font-size:10pt">3. <a href="http://blog.csdn.net/aladdina/archive/2009/04/30/4141094.aspx" target="_blank"><span style="color:#336699">Support vector machines</span></a><br />
		</span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:宋体">支持向量机，英文为</span><span style="font-family:Arial">Support Vector Machine</span><span style="font-family:宋体">，简称</span><span style="font-family:Arial">SV</span><span style="font-family:宋体">机（论文中一般简称</span><span style="font-family:Arial">SVM</span><span style="font-family:宋体">）。它是一种監督式學習的方法，它广泛的应用于统计分类以及回归分析中。支持向量机将向量映射到一个更</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">高维的空间里，在这个空间里建立有一个最大间隔超平面。在分开数据的超平面的两边建有两个互相平行的超平面。分隔超平面使两个平行超平面的距离最大化。假</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">定平行超平面间的距离或差距越大，分类器的总误差越小。一个极好的指南是</span><span style="font-family:Arial">C.J.C Burges</span><span style="font-family:宋体">的《模式识别支持向量机指南》。</span><span style="font-family:Arial">van der Walt </span><span style="font-family:宋体">和</span><span style="font-family:Arial"> Barnard </span><span style="font-family:宋体">将支持向量机和其他分类器进行了比较。</span><span style="font-family:Arial"> <br />
</span></span></p>
<p style="background: white"><span style="color:#333333; font-family:Arial; font-size:10pt">4. <a href="http://blog.csdn.net/aladdina/archive/2009/05/01/4141101.aspx" target="_blank"><span style="color:#336699">The Apriori algorithm</span></a><br />
		</span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">Apriori</span><span style="font-family:宋体">算法是一种最有影响的挖掘布尔关联规则频繁项集的算法。其核心是基于两阶段频集思想的递推算法。该关联规则在分类上属于单维、单层、布尔关联规则。在这里，所有支持度大于最小支持度的项集称为频繁项集，简称频集。</span><span style="font-family:Arial"> <br />
</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">5. <a href="http://blog.csdn.net/aladdina/archive/2009/05/01/4141114.aspx" target="_blank"/></span><span style="color:#336699"><span style="font-family:宋体">最大期望</span><span style="font-family:Arial">(EM)</span><span style="font-family:宋体">算法</span><span style="color:#333333; font-family:Arial"><br />
				</span></span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:宋体">在统计计算中，最大期望（</span><span style="font-family:Arial">EM</span><span style="font-family:宋体">，</span><span style="font-family:Arial">Expectation–Maximization</span><span style="font-family:宋体">）算法是在概率（</span><span style="font-family:Arial">probabilistic</span><span style="font-family:宋体">）模型中寻找参数最大似然</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">估计的算法，其中概率模型依赖于无法观测的隐藏变量（</span><span style="font-family:Arial">Latent Variabl</span><span style="font-family:宋体">）。最大期望经常用在机器学习和计算机视觉的数据集聚（</span><span style="font-family:Arial">Data Clustering</span><span style="font-family:宋体">）领域。</span><span style="font-family:Arial"> <br />
</span></span></p>
<p style="background: white"><span style="color:#333333; font-family:Arial; font-size:10pt">6. <a href="http://blog.csdn.net/aladdina/archive/2009/05/01/4141120.aspx" target="_blank"><span style="color:#336699">PageRank</span></a><br />
		</span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">PageRank</span><span style="font-family:宋体">是</span><span style="font-family:Arial">Google</span><span style="font-family:宋体">算法的重要内容。</span><span style="font-family:Arial">2001</span><span style="font-family:宋体">年</span><span style="font-family:Arial">9</span><span style="font-family:宋体">月被授予美国专利，专利人是</span><span style="font-family:Arial">Google</span><span style="font-family:宋体">创始人之一拉里</span><span style="font-family:Arial">·</span><span style="font-family:宋体">佩奇（</span><span style="font-family:Arial">Larry Page</span><span style="font-family:宋体">）。因此，</span><span style="font-family:Arial">PageRank</span><span style="font-family:宋体">里的</span><span style="font-family:Arial">page</span><span style="font-family:宋体">不是指网页，而是指佩奇，即这个等级方法是以佩奇来命名的。</span><span style="font-family:Arial"><br />
			</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">PageRank</span><span style="font-family:宋体">根据网站的外部链接和内部链接的数量和质量俩衡量网站的价值。</span><span style="font-family:Arial">PageRank</span><span style="font-family:宋体">背后的概念是，每个到页面的链接都是对该页面的一次投票，</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">被链接的越多，就意味着被其他网站投票越多。这个就是所谓的</span><span style="font-family:Arial">&#8220;</span><span style="font-family:宋体">链接流行度</span><span style="font-family:Arial">&#8220;——</span><span style="font-family:宋体">衡量多少人愿意将他们的网站和你的网站挂钩。</span><span style="font-family:Arial">PageRank</span><span style="font-family:宋体">这个概念引自</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">学术中一篇论文的被引述的频度</span><span style="font-family:Arial">——</span><span style="font-family:宋体">即被别人引述的次数越多，一般判断这篇论文的权威性就越高。</span><span style="font-family:Arial"> <br />
</span></span></p>
<p style="background: white"><span style="color:#333333; font-family:Arial; font-size:10pt">7. <a href="http://blog.csdn.net/aladdina/archive/2009/05/01/4141124.aspx" target="_blank"><span style="color:#336699">AdaBoost</span></a><br />
		</span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">Adaboost</span><span style="font-family:宋体">是一种迭代算法，其核心思想是针对同一个训练集训练不同的分类器</span><span style="font-family:Arial">(</span><span style="font-family:宋体">弱分类器</span><span style="font-family:Arial">)</span><span style="font-family:宋体">，然后把这些弱分类器集合起来，构成一个更强的最终分类器</span><span style="font-family:Arial"> (</span><span style="font-family:宋体">强分类器</span><span style="font-family:Arial">)</span><span style="font-family:宋体">。其算法本身是通过改变数据分布来实现的，它根据每次训练集之中每个样本的分类是否正确，以及上次的总体分类的准确率，来确定每个样本的权</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">值。将修改过权值的新数据集送给下层分类器进行训练，最后将每次训练得到的分类器最后融合起来，作为最后的决策分类器。</span><span style="font-family:Arial"><br />
			</span></span></p>
<p style="background: white"><span style="color:#333333; font-family:Arial; font-size:10pt">8. <a href="http://blog.csdn.net/aladdina/archive/2009/05/01/4141127.aspx" target="_blank"><span style="color:#336699">kNN: k-nearest neighbor classification</span></a><br />
		</span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">K</span><span style="font-family:宋体">最近邻</span><span style="font-family:Arial">(k-Nearest Neighbor</span><span style="font-family:宋体">，</span><span style="font-family:Arial">KNN)</span><span style="font-family:宋体">分类算法，是一个理论上比较成熟的方法，也是最简单的机器学习算法之一。该方法的思路是：如果一个样本在特征空间中的</span><span style="font-family:Arial">k</span><span style="font-family:宋体">个最相似</span><span style="font-family:Arial">(</span><span style="font-family:宋体">即特征空间中最邻近</span><span style="font-family:Arial">)</span><span style="font-family:宋体">的样本中的大多数属于某一个类别，则该样本也属于这个类别。</span><span style="font-family:Arial"><br />
			</span></span></p>
<p style="background: white"><span style="color:#333333; font-family:Arial; font-size:10pt">9. <a href="http://blog.csdn.net/aladdina/archive/2009/05/01/4141140.aspx" target="_blank"><span style="color:#336699">Naive Bayes</span></a><br />
		</span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:宋体">在众多的分类模型中，应用最为广泛的两种分类模型是决策树模型</span><span style="font-family:Arial">(Decision Tree Model)</span><span style="font-family:宋体">和朴素贝叶斯模型（</span><span style="font-family:Arial">Naive Bayesian Model</span><span style="font-family:宋体">，</span><span style="font-family:Arial">NBC</span><span style="font-family:宋体">）。</span><span style="font-family:Arial"><strong> </strong></span><span style="font-family:宋体"><strong>朴素贝叶斯模型</strong>发源于古典数学理论，有着坚实的数学基础，以</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">及稳定的分类效率。同时，</span><span style="font-family:Arial">NBC</span><span style="font-family:宋体">模型所需估计的参数很少，对缺失数据不太敏感，算法也比较简单。理论上，</span><span style="font-family:Arial">NBC</span><span style="font-family:宋体">模型与其他分类方法相比具有最小的误差率。</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">但是实际上并非总是如此，这是因为</span><span style="font-family:Arial">NBC</span><span style="font-family:宋体">模型假设属性之间相互独立，这个假设在实际应用中往往是不成立的，这给</span><span style="font-family:Arial">NBC</span><span style="font-family:宋体">模型的正确分类带来了一定影响。在属</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">性个数比较多或者属性之间相关性较大时，</span><span style="font-family:Arial">NBC</span><span style="font-family:宋体">模型的分类效率比不上决策树模型。而在属性相关性较小时，</span><span style="font-family:Arial">NBC</span><span style="font-family:宋体">模型的性能最为良好。</span><span style="font-family:Arial"> <br />
</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">10. <a href="http://blog.csdn.net/aladdina/archive/2009/05/01/4141150.aspx" target="_blank"><span style="color:#336699">CART: </span></a></span><span style="font-family:宋体">分类与回归树</span><span style="font-family:Arial"><br />
			</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">CART, Classification and Regression Trees</span><span style="font-family:宋体">。</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">在分类树下面有两个关键的思想。第一个是关于递归地划分自变量空间的想法；第二个想法是用验证数据进行剪枝。</span><span style="font-family:Arial"><br />
			</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:宋体">本文来源：</span><span style="font-family:Arial">http://blog.csdn.net/aladdina/<br />
</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:宋体">上面的</span><span style="font-family:Arial">10</span><span style="font-family:宋体">篇文章的摘要来源全部转载自网络搜索，百度百科内容最多，少量来自中文维基百科以及其他网页。</span><span style="font-family:Arial"><br />
			</span></span></p>
]]></content:encoded>
			<wfw:commentRss>http://wp.hhy/?feed=rss2&#038;p=17</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>决策树系列算法</title>
		<link>http://wp.hhy/?p=15</link>
		<comments>http://wp.hhy/?p=15#comments</comments>
		<pubDate>Sat, 04 Oct 2014 04:49:55 +0000</pubDate>
		<dc:creator><![CDATA[huahuiyou]]></dc:creator>
				<category><![CDATA[数据挖掘]]></category>
		<category><![CDATA[C4.5]]></category>
		<category><![CDATA[CART]]></category>
		<category><![CDATA[ID3]]></category>
		<category><![CDATA[决策树]]></category>

		<guid isPermaLink="false">http://wp.hhy/?p=15</guid>
		<description><![CDATA[ID3算法 ID3算法的核心：用信息增益作为属性选择的标准。 检测所有属性，用信<span class="ellipsis">&#8230;</span><div class="read-more"><a href="http://wp.hhy/?p=15">查阅全文 &#8250;</a></div><!-- end of .read-more -->]]></description>
				<content:encoded><![CDATA[<p>ID3算法
<p>ID3算法的核心：用信息增益作为属性选择的标准。
<p>检测所有属性，用信息增益最大的属性产生决策树的节点，该节点的不同取值建立分支。再对各分支的数据子集调用决策树算法。
<p>直到所有子集仅包含同一类别的数据为止。
<p>缺点：只对小数据集有效，噪声敏感。
<p>信息增益选择属性偏向取值多的属性；
<p>决策树节点的构造：
<p>节点：决策属性名称，决策条件，决策规则，一般采用连续属性离散化去属性值
<p>决策规则判断函数：属于该节点时，往下探查。
<p>ID3算法的思路：算法描述
<p>输入：属性集合，带类别的数据集合
<p>输出决策树：决策树的头结点，见决策树构造
<p>1.从数据集提取类别集合：类别名称，类别数量（每个类别的概率），每个属性的值列表，取值对应每个类别的数量（概率），
<p>2.计算数据集的系统熵：ES，如果ES=0，则该节点为叶子节点，类别为数据集类别
<p><img title="clip_image001" style="border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px" border="0" alt="clip_image001" src="http://wp.hhy/wp-content/uploads/2014/10/clip_image001.png" width="244" height="18">
<p>：用概率乘以概率的对数，再每个类别相减。
<p>3.对于每一个属性A，计算信息熵EA
<p>3.1对于每一个取值AV，计算该取值的信息熵EAV：对应每个类别均有概率，用2方法计算信息熵，Pn代表该取值对应的属于类别n的概率。
<p>3.2加权平均属性A的所有取值的信息熵，EA=∑权重（A属性取该值的概率）*该值的EAV
<p>4.求信息增益：EAZ=ES-EA；在所有属性中取信息增益最大的作为决策节点，该节点的不同取值作为分支。
<p>5.递归建立分支节点：对于A=V，选择对应的数据子集，剔除A属性，对该数据集采用ID3算法，
<p>6.返回4中建立的头结点。
<p>ID3分类思路：算法描述
<p>输入： ID3决策树，分类样本。
<p>输出：分类标签
<p>1.如果ID头结点是叶子节点，返回分类标签的值。
<p>2.读取头结点属性标签，以及分类样本对应属性值
<p>3.根据对应属性值，查找对应的分支。
<p>4.递归调用ID3分类函数，输入分支树、待分类样本。
<p>5.return 递归返回值。
<p>参考资料：
<p><a href="http://blog.csdn.net/guoqiangma/article/details/7188678">http://blog.csdn.net/guoqiangma/article/details/7188678</a>
<p><a href="http://www.cnblogs.com/zhangchaoyang/articles/2196631.html">http://www.cnblogs.com/zhangchaoyang/articles/2196631.html</a>
<p><a href="http://czhsuccess.iteye.com/blog/1864652">http://czhsuccess.iteye.com/blog/1864652</a>
<p>机器学习十大算法（1）C4.5
<p>一、原理
<p>对ID3算法的改进：
<p>1.用信息增益率来选择属性；熵的变化值：把每个属性看成是类别，用信息熵公式计算信息熵HV（同ID3中的第二步），然后用ID3中的信息增益除以HV，得到信息增益率。和ID3中第3步方法不同，不考虑属性值对应类别的熵，只是属性一个维度的熵。
<p>2.决策树构造过程中，进行剪枝，考虑样本少的节点，会产生过拟合问题。
<p>3.可以处理离散型数据。
<p>4.对不完整数据进行处理。
<p>连续属性的处理：
<p>在选择某节点的分支属性时，对于离散型属性，按ID3的算法算增益，然后再算增益率。
<p>对于连续型的属性，则需要对其进行离散化处理：
<p>1.将连续属性值从小到大排序：AV1，AV2,…AVN
<p>2.对1序列可以分成N-1个分割点，也就是有N-1种划分成2份的方法。
<p>3.在N-1种分割方法中寻找一种最佳分割：按C4.5信息增益率计算出N-1中分割的信息增益率，取最大的那个作为该属性的分割，和信息增益率。
<p>后剪枝处理：
<p>避免树的高度无节制增长，避免过度拟合数据。
<p>估计剪枝前后的误差，决定是否剪枝。
<p>Pr[ (−)/( (1−)/) &gt;]= c
<p>其中N是实例的数量，f=E/N为观察到的误差率（其中E为N个实例中分类错误的个数），q为真实的误差率，c为置信度（C4.5算法的一个熟人参数，默认值为0.25），z为对应于置信度c的标准差，其值可根据c的设定值通过查正态分布表得到。
<p>通过上公式可以计算出真实误差率q的一个置信区间上限，用此上限为该节点误差率e做一个悲观的估计：
<p><img title="clip_image002" style="border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px" border="0" alt="clip_image002" src="http://wp.hhy/wp-content/uploads/2014/10/clip_image002.png" width="223" height="106">
<p>根据剪枝前后e的值，从而决定是否需要剪枝。
<p>缺失值的处理：
<p>针对数据缺少某些属性值的情况。
<p>策略一：对于赋予该值为该属性常见的值。
<p>根据一个概率分配所有缺少该属性值的样本一个属性值，使得该属性值分布和之前一致。
<p>C4.5生成算法：
<p>输入：数据集DataSet
<p>输出：决策树根节点
<p>1.创建根节点N
<p>2.计算类别的信息熵，同ID3算法：统计类别标签序列以及属性值序列，&lt;属性值，类别&gt;序列的数量，对于属性值缺失的地方，采用策略一或者策略二进行补充。
<p>3.如果信息熵为0，则设置节点为叶子节点，类别为数据集类别。
<p>4.对于每个属性，计算增益率
<p>4.1如果属性是离散属性，则按ID3算法计算增益，按类别信息熵计算方法计算属性信息熵。最终得到增益率
<p>4.2如果属性是连续属性，则将属性值从小到大排序，N个不同的属性值。
<p>4.2.1在N-1个位置分别将属性分成两部分，离散的属性值。
<p>4.2.2按4.1的方法计算信息增益率。
<p>4.2.3取最大的信息增益率和划分位置，作为该属性的信息增益率。
<p>附：
<p>5.在所有属性增益率中选择最大增益率的属性，作为根节点的属性标签。属性值作为分支条件。
<p>6.递归下一个节点：根据每个属性值构建分支节点，属性值满足对应分支条件的数据当中输入踢去根节点属性的数据集。
<p>7.计算每个节点的分类错误，进行剪枝。？？？剪枝后如何处理
<p>8.return 根节点
<p>C4.5分类算法：同ID3
<p>参考资料：
<p><a href="http://blog.csdn.net/xuxurui007/article/details/18045943">http://blog.csdn.net/xuxurui007/article/details/18045943</a>
<p><a href="http://www.cnblogs.com/zhangchaoyang/articles/2842490.html">http://www.cnblogs.com/zhangchaoyang/articles/2842490.html</a>
<p><a href="http://wenku.baidu.com/link?url=G7mc7ywnFgAc62PAIVkSqVHbS3TRYrN0rI-q8tspmgnDFGRngeoKCM_FPL31s9K7G4RoA45fZzeFNyube_LzhZfrkjY8XO41MkiINtyZqfa">http://wenku.baidu.com/link?url=G7mc7ywnFgAc62PAIVkSqVHbS3TRYrN0rI-q8tspmgnDFGRngeoKCM_FPL31s9K7G4RoA45fZzeFNyube_LzhZfrkjY8XO41MkiINtyZqfa</a>
<p>weka.classifiers.trees.j48
<p>机器学习十大算法（10）CART
<p>Classification and regression trees
<p>特点:采用二分递归分割技术，将当前样本分成两个子集，使得生成的每个非叶子节点都有两个分支。二叉树。
<p>针对离散属性进行分类，针对连续属性则进行回归。
<p>数据划分策略：Gini指标
<p><img title="clip_image003" style="border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px" border="0" alt="clip_image003" src="http://wp.hhy/wp-content/uploads/2014/10/clip_image003.gif" width="177" height="49">
<p>计算方式：
<p>对于每一个属性，划分成两部分，每部分数据中，计算Gini指标，然后用加权平均两部分的GinI指标作为该属性的Gini指标。权值为该部分数据所占的比例。
<p>在所有属性中选择属性Gini指标最小的一个作为决策节点，由此数据集被分成较纯的两部分。
<p>关键：根据属性值，划分成两部分，这个真子集比较难选，2^N种可能。
<p>有个问题：如果一个属性中的两个值的区别是区分某两个或两类样本的关键，在该属性的二元划分中没有将其划分开来，那怎么办？
<p>参考资料：
<p><a href="http://blog.csdn.net/hewei0241/article/details/8280490">http://blog.csdn.net/hewei0241/article/details/8280490</a>
<p><a href="http://blog.sina.com.cn/s/blog_7399ad1f01014oic.html">http://blog.sina.com.cn/s/blog_7399ad1f01014oic.html</a></p>
]]></content:encoded>
			<wfw:commentRss>http://wp.hhy/?feed=rss2&#038;p=15</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>ZBlog和GitHub结合管理静态博客系统</title>
		<link>http://wp.hhy/?p=10</link>
		<comments>http://wp.hhy/?p=10#comments</comments>
		<pubDate>Sat, 04 Oct 2014 04:41:00 +0000</pubDate>
		<dc:creator><![CDATA[huahuiyou]]></dc:creator>
				<category><![CDATA[网站]]></category>
		<category><![CDATA[GitHub]]></category>
		<category><![CDATA[博客]]></category>

		<guid isPermaLink="false">http://wp.hhy/?p=10</guid>
		<description><![CDATA[问题描述： gitHub支持静态的网站托管，但是目前还没有一套堪比动态博客的静态<span class="ellipsis">&#8230;</span><div class="read-more"><a href="http://wp.hhy/?p=10">查阅全文 &#8250;</a></div><!-- end of .read-more -->]]></description>
				<content:encoded><![CDATA[<p>问题描述：
<p>gitHub支持静态的网站托管，但是目前还没有一套堪比动态博客的静态博客系统。
<p>已有的一些维护起来不太方便，为此笔者探索了一套GIT管理静态博客方案。
<p>思路和步骤：
<p>1.用ZBlog 管理博客，分类，标签，<strong>搜索</strong>等等。
<p>2.用HTTrackPortable.exe 全站生成静态网站。
<p>3.GitHub自带工具，发布到Github仓库中。当中出现目录不同步的问题，可以用SyncToy 2.1(x64)进行2和3两个目录的同步。
<p>部署过程中出现的问题：
<p>1.博客系统的选择
<p>选择了一个支持伪静态的博客系统。
<p>2.整站下载工具的选择
<p>整站下载工具，有好有坏，主要问题是页面中JS代码的加载问题。目前选择了一个较好的工具。
<p>但是JS的延迟加载还是会出现问题。
<p>为此为了保证静态博客的有效性，需要对没有内容的部分进行隐藏和替换。</p>
]]></content:encoded>
			<wfw:commentRss>http://wp.hhy/?feed=rss2&#038;p=10</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>数据挖掘测试</title>
		<link>http://wp.hhy/?p=5</link>
		<comments>http://wp.hhy/?p=5#comments</comments>
		<pubDate>Fri, 03 Oct 2014 14:01:23 +0000</pubDate>
		<dc:creator><![CDATA[huahuiyou]]></dc:creator>
				<category><![CDATA[数据挖掘]]></category>

		<guid isPermaLink="false">http://wp.hhy/?p=5</guid>
		<description><![CDATA[测试数据挖掘]]></description>
				<content:encoded><![CDATA[<p>测试数据挖掘</p>
]]></content:encoded>
			<wfw:commentRss>http://wp.hhy/?feed=rss2&#038;p=5</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>世界，你好！</title>
		<link>http://wp.hhy/?p=1</link>
		<comments>http://wp.hhy/?p=1#comments</comments>
		<pubDate>Fri, 03 Oct 2014 13:34:38 +0000</pubDate>
		<dc:creator><![CDATA[huahuiyou]]></dc:creator>
				<category><![CDATA[未分类]]></category>

		<guid isPermaLink="false">http://wp.hhy/?p=1</guid>
		<description><![CDATA[欢迎使用WordPress。这是系统自动生成的演示文章。编辑或者删除它，然后开始<span class="ellipsis">&#8230;</span><div class="read-more"><a href="http://wp.hhy/?p=1">查阅全文 &#8250;</a></div><!-- end of .read-more -->]]></description>
				<content:encoded><![CDATA[<p>欢迎使用WordPress。这是系统自动生成的演示文章。编辑或者删除它，然后开始您的博客！</p>
]]></content:encoded>
			<wfw:commentRss>http://wp.hhy/?feed=rss2&#038;p=1</wfw:commentRss>
		<slash:comments>1</slash:comments>
		</item>
	</channel>
</rss>
