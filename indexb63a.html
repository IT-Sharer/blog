<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>IT-Sharer &#187; 数据挖掘</title>
	<atom:link href="http://wp.hhy/?feed=rss2&#038;cat=2" rel="self" type="application/rss+xml" />
	<link>http://wp.hhy</link>
	<description>温 润 醇 和    臻 于 至 善</description>
	<lastBuildDate>Fri, 10 Oct 2014 06:22:24 +0000</lastBuildDate>
	<language>zh-CN</language>
		<sy:updatePeriod>hourly</sy:updatePeriod>
		<sy:updateFrequency>1</sy:updateFrequency>
	<generator>http://wordpress.org/?v=4.0</generator>
	<item>
		<title>数据挖掘原语-形式化定义过程描述</title>
		<link>http://wp.hhy/?p=77</link>
		<comments>http://wp.hhy/?p=77#comments</comments>
		<pubDate>Thu, 09 Oct 2014 09:21:06 +0000</pubDate>
		<dc:creator><![CDATA[huahuiyou]]></dc:creator>
				<category><![CDATA[数据挖掘]]></category>

		<guid isPermaLink="false">http://wp.hhy/?p=77</guid>
		<description><![CDATA[数据挖掘即是一个需要人工干预和指导，又是一个高度流程化的一个过程。这就使得在数据<span class="ellipsis">&#8230;</span><div class="read-more"><a href="http://wp.hhy/?p=77">查阅全文 &#8250;</a></div><!-- end of .read-more -->]]></description>
				<content:encoded><![CDATA[<p>数据挖掘即是一个需要人工干预和指导，又是一个高度流程化的一个过程。这就使得在数据挖掘系统设计中需要考虑用用于交互的模块，也要将流程化的模块固定下来。
</p>
<p>数据挖掘挖掘原语主要解决的就是上述问题。原语不仅定义了数据挖掘的要素和主要流程，还定义了每个流程和要素模块，用户是如何介入的。
</p>
<p>数据挖掘原语组成：任务相关的数据，要挖掘的知识类型，指导挖掘过程的背景知识，模式评估兴趣度量和显示所发现的知识。
</p>
<ol>
<li>
<div style="text-align: justify"><span style="font-size:10pt">任务相关的数据：定义数据输入的来源，格式。常常由用户选定数据集。<br />
</span></div>
</li>
<li>
<div style="text-align: justify"><span style="font-size:10pt">要挖掘的知识类型：描述和区分，分类和预测，聚类，异常点检测，时序模型等等。通常知识类型描述的是需要的输出结果，例如一个统计数据的方法、分类器，异常检测器等等。可以利用这些算法（机）输出知识信息。<br />
</span></div>
</li>
<li>
<div style="text-align: justify"><span style="font-size:10pt">指导的背景知识：已有的一些知识。例如挖掘对象的语义模型，概念分层模型。属性间的关联规则等等。<br />
</span></div>
</li>
<li>
<div style="text-align: justify"><span style="font-size:10pt">兴趣度量：对挖掘结果的评估和筛选。所有数据挖掘结果并不一定是有用的，合理的。<br />
</span></div>
</li>
<li>
<div style="text-align: justify"><span style="font-size:10pt">知识可视化：图表，多维度展示，规则库等等方式。<br />
</span></div>
<p style="margin-left: 21pt">
 </p>
<p style="margin-left: 21pt"><strong>相关产品：<br />
</strong></p>
<p>SQLSERVER Analysis服务系列：可以从数据仓库中选择一个数据立方或者多个数据集，然后选择指定的算法进行挖掘。挖掘结果可以用决策树，图表等多种展现方式。
</p>
<p>Weka平台：支持数据的导入、清洗，数据挖掘方法的选择，结果可视化等。
</p>
<p>
 </p>
<p><strong>研究点：</strong>数据挖掘查询语言DMQL
</p>
<p>目前对用户指导的背景知识设计这块还比较欠缺。大多数产品都是选择模式进行数据的处理。而没有采取融合多个算法模式进行挖掘。
</p>
<p>背景知识利用的不充分。用户可以自定义的背景知识较为有限。
</p>
<p><img src="http://wp.hhy/wp-content/uploads/2014/10/100914_0921_1.png" alt=""/></p>
</li>
</ol>
]]></content:encoded>
			<wfw:commentRss>http://wp.hhy/?feed=rss2&#038;p=77</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>数据预处理概述</title>
		<link>http://wp.hhy/?p=46</link>
		<comments>http://wp.hhy/?p=46#comments</comments>
		<pubDate>Tue, 07 Oct 2014 09:07:07 +0000</pubDate>
		<dc:creator><![CDATA[huahuiyou]]></dc:creator>
				<category><![CDATA[数据挖掘]]></category>

		<guid isPermaLink="false">http://wp.hhy/?p=46</guid>
		<description><![CDATA[在数据挖掘整个项目中，数据准备（采集，预处理）可能包含了整个项目进度的20%左右<span class="ellipsis">&#8230;</span><div class="read-more"><a href="http://wp.hhy/?p=46">查阅全文 &#8250;</a></div><!-- end of .read-more -->]]></description>
				<content:encoded><![CDATA[<p>在数据挖掘整个项目中，数据准备（采集，预处理）可能包含了整个项目进度的20%左右，主要问题是清洗、转换、集成、和归约。</p>
<p><strong>1.数据预处理的目的，对应方法</strong></p>
<p>对不完整的，含噪音的和不一致的数据进行处理；====》数据清理</p>
<p>对数据进行转换，以满足数据挖掘算法的需求；=====》数据转换</p>
<p>数据集成：将多源异构的数据集成到一起，增加样本多样性；===》数据集成</p>
<p>聚集、删除冗余特征，压缩数据。====》数据归约</p>
<p><strong>2.数据清理</strong></p>
<p><strong>2.1</strong>缺失值处理</p>
<ul>
<ul>
<li>忽略该样本：多个主要属性缺失，该实例为无效实例；</li>
<li>人工填写遗漏值：少量缺失，可以人工填充；</li>
<li>使用默认值填充：设置对应属性的默认值；</li>
<li>使用属性的平均值；</li>
<li>使用给定样本所属类别中的该属性的平均值；</li>
<li>时间序列样本，采用平滑拟合后该时间点出的值；</li>
</ul>
</ul>
<p>2.2噪音数据处理</p>
<blockquote><p>分箱：考察邻居的值，平滑数据。</p>
<p>将数据划分成若干小的箱子，在箱子内部可以采用按平均值、中值、边界平滑等策略</p>
<p>聚类：将样本数据按照聚类规则聚类，满足一定相似度条件的数据平滑</p>
<p>人工处理</p>
<p>回归：使用回归函数，平滑数据。</p>
</blockquote>
<p>2.3不一致数据</p>
<p>根据一致性规则，找出不符合规则的样本，进行更正。</p>
<p><strong>3.数据集成和变换</strong></p>
<p>3.1集成：主要考虑异源异构的数据集之间的集成，结构统一问题</p>
<p>3.2变换</p>
<blockquote><p>数据平滑：分箱平滑、聚类和回归；</p>
<p>规范化：最小-最大规范化；Z-Score规范化；</p>
</blockquote>
<p><strong>4.数据归约</strong></p>
<p>4.1属性选择</p>
<blockquote><p>逐步向前选择：从属性集中选择最好的属性，加入待选属性，直到属性不满足条件停止。决策树系列。</p>
<p>逐步向后剔除：从属性集中剔除最坏的属性，可以用信息熵或者信息增益衡量好坏。</p>
<p>向前选择和向后剔除结合：结合前两种方式。</p>
</blockquote>
<p>4.2数据压缩</p>
<blockquote><p>小波变换：使得数据波在一段长度内变得平滑。从而只保留少部分参数就可以保存整段数据。</p>
</blockquote>
<p>4.3主成分分析PCA</p>
<p>求得一个最小的线性无关组合。</p>
<p>4.4数值归约</p>
<p>用一个规则保存一整段的数据，而不是一个一个数据保存。</p>
<p>分箱平滑，聚集，回归等。</p>
<p>4.5数据抽样</p>
<p>不放回抽样，放回抽样，聚类抽样，分层抽样，等比例分类抽样。</p>
<p>4.6离散化与概念分层</p>
<blockquote><p>连续属性的离散化：分箱（等距离散化）、聚类分析、基于信息增益的离散化（C4.5算法中采用）、自然划分（3-4-5规则）</p>
</blockquote>
]]></content:encoded>
			<wfw:commentRss>http://wp.hhy/?feed=rss2&#038;p=46</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>数据挖掘的任务和功能</title>
		<link>http://wp.hhy/?p=42</link>
		<comments>http://wp.hhy/?p=42#comments</comments>
		<pubDate>Tue, 07 Oct 2014 03:35:00 +0000</pubDate>
		<dc:creator><![CDATA[huahuiyou]]></dc:creator>
				<category><![CDATA[数据挖掘]]></category>
		<category><![CDATA[基础]]></category>

		<guid isPermaLink="false">http://wp.hhy/?p=42</guid>
		<description><![CDATA[数据挖掘的任务一般可以分为两类:描述和预测。描述性任务一般刻画数据中的一般特性，<span class="ellipsis">&#8230;</span><div class="read-more"><a href="http://wp.hhy/?p=42">查阅全文 &#8250;</a></div><!-- end of .read-more -->]]></description>
				<content:encoded><![CDATA[<p>数据挖掘的任务一般可以分为两类:描述和预测。描述性任务一般刻画数据中的一般特性，例如统计等；预测性挖掘任务在当前数据上进行推断和预测。</p>
<p>数据挖掘的功能常见如下：</p>
<p>1.概念/类描述:特征和区分</p>
<p>描述数据的特征,汇总统计,多维度展现，概念比较等手段。</p>
<p>2.关联分析</p>
<p>发现数据事务中的频繁模式。有较高的支持度和置信度。</p>
<p>3.分类和预测</p>
<p>对&lt;x,y&gt;的数据建立分类函数F，使得对于xi可以求得y=f(xi)，</p>
<p>其中x是实例的特征表示，y是实例的类标号。</p>
<p>4.聚类分析</p>
<p>实例根据最大化类内相似性、最小化类间相似性的原则进行聚类或分组。</p>
<p>常常也利用聚类的这一原则进行决策支持。</p>
<p>5.异常点检测</p>
<p>发现实例群形成一般化特征以外的异常实例。常常使用偏差法。引入一定偏差变量，例如SVM中的松弛变量。</p>
<p>6.演变分析</p>
<p>描述数据随时间变化的规律或者趋势。</p>
<p>常常是时间序列分析。</p>
<p>也有偏序关系分析：事务关联分析中两个事务具有时间顺序的关系。</p>
]]></content:encoded>
			<wfw:commentRss>http://wp.hhy/?feed=rss2&#038;p=42</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>数据挖掘十大算法</title>
		<link>http://wp.hhy/?p=17</link>
		<comments>http://wp.hhy/?p=17#comments</comments>
		<pubDate>Sat, 04 Oct 2014 05:01:37 +0000</pubDate>
		<dc:creator><![CDATA[huahuiyou]]></dc:creator>
				<category><![CDATA[数据挖掘]]></category>

		<guid isPermaLink="false">http://wp.hhy/?p=17</guid>
		<description><![CDATA[国际权威的学术组织the IEEE International Conferen<span class="ellipsis">&#8230;</span><div class="read-more"><a href="http://wp.hhy/?p=17">查阅全文 &#8250;</a></div><!-- end of .read-more -->]]></description>
				<content:encoded><![CDATA[<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:宋体">国际权威的学术组织</span><span style="font-family:Arial">the IEEE International Conference on Data Mining (ICDM) 2006</span><span style="font-family:宋体">年</span><span style="font-family:Arial">12</span><span style="font-family:宋体">月评选出了数据挖掘领域的十大经典算法：</span><span style="font-family:Arial">C4.5, k-Means, SVM, Apriori, EM, PageRank, AdaBoost, kNN, Naive Bayes, and CART.<br />
</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:宋体">不仅仅是选中的十大算法，其实参加<span style="color:black">评选的</span></span><span style="font-family:Arial">18</span><span style="color:black"><span style="font-family:宋体">种算法，实际上随便拿出一种来都可以称得上是经典算法，它们在数据挖掘领域都产生了极为深远的影响。</span><span style="color:#333333; font-family:Arial"> <br />
</span></span></span></p>
<p style="background: white"><span style="color:black; font-family:Arial; font-size:10pt">1. <a href="http://blog.csdn.net/aladdina/archive/2009/04/30/4141048.aspx" target="_blank"><span style="color:#336699">C4.5</span></a><span style="color:#333333"><br />
			</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">C4.5</span><span style="font-family:宋体">算法是机器学习算法中的一种分类决策树算法</span><span style="font-family:Arial">,</span><span style="font-family:宋体">其核心算法是</span><span style="font-family:Arial">ID3</span><span style="font-family:宋体">算法</span><span style="font-family:Arial">.  C4.5</span><span style="font-family:宋体">算法继承了</span><span style="font-family:Arial">ID3</span><span style="font-family:宋体">算法的优点，并在以下几方面对</span><span style="font-family:Arial">ID3</span><span style="font-family:宋体">算法进行了改进：</span><span style="font-family:Arial"><br />
			</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">1) </span><span style="font-family:宋体">用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足；</span><span style="font-family:Arial"><br/>    2) </span><span style="font-family:宋体">在树构造过程中进行剪枝；</span><span style="font-family:Arial"><br/>    3) </span><span style="font-family:宋体">能够完成对连续属性的离散化处理；</span><span style="font-family:Arial"><br/>    4) </span><span style="font-family:宋体">能够对不完整数据进行处理。</span><span style="font-family:Arial"><br />
			</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">C4.5</span><span style="font-family:宋体">算法有如下优点：产生的分类规则易于理解，准确率较高。其缺点是：在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。</span><span style="font-family:Arial"> <br />
</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">2. <a href="http://blog.csdn.net/aladdina/archive/2009/04/30/4141089.aspx" target="_blank"><span style="color:#336699">The k-means algorithm</span></a> </span><span style="font-family:宋体">即</span><span style="font-family:Arial">K-Means</span><span style="font-family:宋体">算法</span><span style="font-family:Arial"><br />
			</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">k-means algorithm</span><span style="font-family:宋体">算法是一个聚类算法，把</span><span style="font-family:Arial">n</span><span style="font-family:宋体">的对象根据他们的属性分为</span><span style="font-family:Arial">k</span><span style="font-family:宋体">个分割，</span><span style="font-family:Arial">k &lt; n</span><span style="font-family:宋体">。它与处理混合正态分布的最大期望算法很相似，因为他们都试图找到数据中自然聚类的中心。它假设对象属性来自于空间向量，并且目标是使各个群组内部的均</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">方误差总和最小。</span><span style="font-family:Arial"> <br />
</span></span></p>
<p style="background: white"><span style="color:#333333; font-family:Arial; font-size:10pt">3. <a href="http://blog.csdn.net/aladdina/archive/2009/04/30/4141094.aspx" target="_blank"><span style="color:#336699">Support vector machines</span></a><br />
		</span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:宋体">支持向量机，英文为</span><span style="font-family:Arial">Support Vector Machine</span><span style="font-family:宋体">，简称</span><span style="font-family:Arial">SV</span><span style="font-family:宋体">机（论文中一般简称</span><span style="font-family:Arial">SVM</span><span style="font-family:宋体">）。它是一种監督式學習的方法，它广泛的应用于统计分类以及回归分析中。支持向量机将向量映射到一个更</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">高维的空间里，在这个空间里建立有一个最大间隔超平面。在分开数据的超平面的两边建有两个互相平行的超平面。分隔超平面使两个平行超平面的距离最大化。假</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">定平行超平面间的距离或差距越大，分类器的总误差越小。一个极好的指南是</span><span style="font-family:Arial">C.J.C Burges</span><span style="font-family:宋体">的《模式识别支持向量机指南》。</span><span style="font-family:Arial">van der Walt </span><span style="font-family:宋体">和</span><span style="font-family:Arial"> Barnard </span><span style="font-family:宋体">将支持向量机和其他分类器进行了比较。</span><span style="font-family:Arial"> <br />
</span></span></p>
<p style="background: white"><span style="color:#333333; font-family:Arial; font-size:10pt">4. <a href="http://blog.csdn.net/aladdina/archive/2009/05/01/4141101.aspx" target="_blank"><span style="color:#336699">The Apriori algorithm</span></a><br />
		</span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">Apriori</span><span style="font-family:宋体">算法是一种最有影响的挖掘布尔关联规则频繁项集的算法。其核心是基于两阶段频集思想的递推算法。该关联规则在分类上属于单维、单层、布尔关联规则。在这里，所有支持度大于最小支持度的项集称为频繁项集，简称频集。</span><span style="font-family:Arial"> <br />
</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">5. <a href="http://blog.csdn.net/aladdina/archive/2009/05/01/4141114.aspx" target="_blank"/></span><span style="color:#336699"><span style="font-family:宋体">最大期望</span><span style="font-family:Arial">(EM)</span><span style="font-family:宋体">算法</span><span style="color:#333333; font-family:Arial"><br />
				</span></span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:宋体">在统计计算中，最大期望（</span><span style="font-family:Arial">EM</span><span style="font-family:宋体">，</span><span style="font-family:Arial">Expectation–Maximization</span><span style="font-family:宋体">）算法是在概率（</span><span style="font-family:Arial">probabilistic</span><span style="font-family:宋体">）模型中寻找参数最大似然</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">估计的算法，其中概率模型依赖于无法观测的隐藏变量（</span><span style="font-family:Arial">Latent Variabl</span><span style="font-family:宋体">）。最大期望经常用在机器学习和计算机视觉的数据集聚（</span><span style="font-family:Arial">Data Clustering</span><span style="font-family:宋体">）领域。</span><span style="font-family:Arial"> <br />
</span></span></p>
<p style="background: white"><span style="color:#333333; font-family:Arial; font-size:10pt">6. <a href="http://blog.csdn.net/aladdina/archive/2009/05/01/4141120.aspx" target="_blank"><span style="color:#336699">PageRank</span></a><br />
		</span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">PageRank</span><span style="font-family:宋体">是</span><span style="font-family:Arial">Google</span><span style="font-family:宋体">算法的重要内容。</span><span style="font-family:Arial">2001</span><span style="font-family:宋体">年</span><span style="font-family:Arial">9</span><span style="font-family:宋体">月被授予美国专利，专利人是</span><span style="font-family:Arial">Google</span><span style="font-family:宋体">创始人之一拉里</span><span style="font-family:Arial">·</span><span style="font-family:宋体">佩奇（</span><span style="font-family:Arial">Larry Page</span><span style="font-family:宋体">）。因此，</span><span style="font-family:Arial">PageRank</span><span style="font-family:宋体">里的</span><span style="font-family:Arial">page</span><span style="font-family:宋体">不是指网页，而是指佩奇，即这个等级方法是以佩奇来命名的。</span><span style="font-family:Arial"><br />
			</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">PageRank</span><span style="font-family:宋体">根据网站的外部链接和内部链接的数量和质量俩衡量网站的价值。</span><span style="font-family:Arial">PageRank</span><span style="font-family:宋体">背后的概念是，每个到页面的链接都是对该页面的一次投票，</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">被链接的越多，就意味着被其他网站投票越多。这个就是所谓的</span><span style="font-family:Arial">&#8220;</span><span style="font-family:宋体">链接流行度</span><span style="font-family:Arial">&#8220;——</span><span style="font-family:宋体">衡量多少人愿意将他们的网站和你的网站挂钩。</span><span style="font-family:Arial">PageRank</span><span style="font-family:宋体">这个概念引自</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">学术中一篇论文的被引述的频度</span><span style="font-family:Arial">——</span><span style="font-family:宋体">即被别人引述的次数越多，一般判断这篇论文的权威性就越高。</span><span style="font-family:Arial"> <br />
</span></span></p>
<p style="background: white"><span style="color:#333333; font-family:Arial; font-size:10pt">7. <a href="http://blog.csdn.net/aladdina/archive/2009/05/01/4141124.aspx" target="_blank"><span style="color:#336699">AdaBoost</span></a><br />
		</span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">Adaboost</span><span style="font-family:宋体">是一种迭代算法，其核心思想是针对同一个训练集训练不同的分类器</span><span style="font-family:Arial">(</span><span style="font-family:宋体">弱分类器</span><span style="font-family:Arial">)</span><span style="font-family:宋体">，然后把这些弱分类器集合起来，构成一个更强的最终分类器</span><span style="font-family:Arial"> (</span><span style="font-family:宋体">强分类器</span><span style="font-family:Arial">)</span><span style="font-family:宋体">。其算法本身是通过改变数据分布来实现的，它根据每次训练集之中每个样本的分类是否正确，以及上次的总体分类的准确率，来确定每个样本的权</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">值。将修改过权值的新数据集送给下层分类器进行训练，最后将每次训练得到的分类器最后融合起来，作为最后的决策分类器。</span><span style="font-family:Arial"><br />
			</span></span></p>
<p style="background: white"><span style="color:#333333; font-family:Arial; font-size:10pt">8. <a href="http://blog.csdn.net/aladdina/archive/2009/05/01/4141127.aspx" target="_blank"><span style="color:#336699">kNN: k-nearest neighbor classification</span></a><br />
		</span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">K</span><span style="font-family:宋体">最近邻</span><span style="font-family:Arial">(k-Nearest Neighbor</span><span style="font-family:宋体">，</span><span style="font-family:Arial">KNN)</span><span style="font-family:宋体">分类算法，是一个理论上比较成熟的方法，也是最简单的机器学习算法之一。该方法的思路是：如果一个样本在特征空间中的</span><span style="font-family:Arial">k</span><span style="font-family:宋体">个最相似</span><span style="font-family:Arial">(</span><span style="font-family:宋体">即特征空间中最邻近</span><span style="font-family:Arial">)</span><span style="font-family:宋体">的样本中的大多数属于某一个类别，则该样本也属于这个类别。</span><span style="font-family:Arial"><br />
			</span></span></p>
<p style="background: white"><span style="color:#333333; font-family:Arial; font-size:10pt">9. <a href="http://blog.csdn.net/aladdina/archive/2009/05/01/4141140.aspx" target="_blank"><span style="color:#336699">Naive Bayes</span></a><br />
		</span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:宋体">在众多的分类模型中，应用最为广泛的两种分类模型是决策树模型</span><span style="font-family:Arial">(Decision Tree Model)</span><span style="font-family:宋体">和朴素贝叶斯模型（</span><span style="font-family:Arial">Naive Bayesian Model</span><span style="font-family:宋体">，</span><span style="font-family:Arial">NBC</span><span style="font-family:宋体">）。</span><span style="font-family:Arial"><strong> </strong></span><span style="font-family:宋体"><strong>朴素贝叶斯模型</strong>发源于古典数学理论，有着坚实的数学基础，以</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">及稳定的分类效率。同时，</span><span style="font-family:Arial">NBC</span><span style="font-family:宋体">模型所需估计的参数很少，对缺失数据不太敏感，算法也比较简单。理论上，</span><span style="font-family:Arial">NBC</span><span style="font-family:宋体">模型与其他分类方法相比具有最小的误差率。</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">但是实际上并非总是如此，这是因为</span><span style="font-family:Arial">NBC</span><span style="font-family:宋体">模型假设属性之间相互独立，这个假设在实际应用中往往是不成立的，这给</span><span style="font-family:Arial">NBC</span><span style="font-family:宋体">模型的正确分类带来了一定影响。在属</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">性个数比较多或者属性之间相关性较大时，</span><span style="font-family:Arial">NBC</span><span style="font-family:宋体">模型的分类效率比不上决策树模型。而在属性相关性较小时，</span><span style="font-family:Arial">NBC</span><span style="font-family:宋体">模型的性能最为良好。</span><span style="font-family:Arial"> <br />
</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">10. <a href="http://blog.csdn.net/aladdina/archive/2009/05/01/4141150.aspx" target="_blank"><span style="color:#336699">CART: </span></a></span><span style="font-family:宋体">分类与回归树</span><span style="font-family:Arial"><br />
			</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">CART, Classification and Regression Trees</span><span style="font-family:宋体">。</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">在分类树下面有两个关键的思想。第一个是关于递归地划分自变量空间的想法；第二个想法是用验证数据进行剪枝。</span><span style="font-family:Arial"><br />
			</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:宋体">本文来源：</span><span style="font-family:Arial">http://blog.csdn.net/aladdina/<br />
</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:宋体">上面的</span><span style="font-family:Arial">10</span><span style="font-family:宋体">篇文章的摘要来源全部转载自网络搜索，百度百科内容最多，少量来自中文维基百科以及其他网页。</span><span style="font-family:Arial"><br />
			</span></span></p>
]]></content:encoded>
			<wfw:commentRss>http://wp.hhy/?feed=rss2&#038;p=17</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>决策树系列算法</title>
		<link>http://wp.hhy/?p=15</link>
		<comments>http://wp.hhy/?p=15#comments</comments>
		<pubDate>Sat, 04 Oct 2014 04:49:55 +0000</pubDate>
		<dc:creator><![CDATA[huahuiyou]]></dc:creator>
				<category><![CDATA[数据挖掘]]></category>
		<category><![CDATA[C4.5]]></category>
		<category><![CDATA[CART]]></category>
		<category><![CDATA[ID3]]></category>
		<category><![CDATA[决策树]]></category>

		<guid isPermaLink="false">http://wp.hhy/?p=15</guid>
		<description><![CDATA[ID3算法 ID3算法的核心：用信息增益作为属性选择的标准。 检测所有属性，用信<span class="ellipsis">&#8230;</span><div class="read-more"><a href="http://wp.hhy/?p=15">查阅全文 &#8250;</a></div><!-- end of .read-more -->]]></description>
				<content:encoded><![CDATA[<p>ID3算法
<p>ID3算法的核心：用信息增益作为属性选择的标准。
<p>检测所有属性，用信息增益最大的属性产生决策树的节点，该节点的不同取值建立分支。再对各分支的数据子集调用决策树算法。
<p>直到所有子集仅包含同一类别的数据为止。
<p>缺点：只对小数据集有效，噪声敏感。
<p>信息增益选择属性偏向取值多的属性；
<p>决策树节点的构造：
<p>节点：决策属性名称，决策条件，决策规则，一般采用连续属性离散化去属性值
<p>决策规则判断函数：属于该节点时，往下探查。
<p>ID3算法的思路：算法描述
<p>输入：属性集合，带类别的数据集合
<p>输出决策树：决策树的头结点，见决策树构造
<p>1.从数据集提取类别集合：类别名称，类别数量（每个类别的概率），每个属性的值列表，取值对应每个类别的数量（概率），
<p>2.计算数据集的系统熵：ES，如果ES=0，则该节点为叶子节点，类别为数据集类别
<p><img title="clip_image001" style="border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px" border="0" alt="clip_image001" src="http://wp.hhy/wp-content/uploads/2014/10/clip_image001.png" width="244" height="18">
<p>：用概率乘以概率的对数，再每个类别相减。
<p>3.对于每一个属性A，计算信息熵EA
<p>3.1对于每一个取值AV，计算该取值的信息熵EAV：对应每个类别均有概率，用2方法计算信息熵，Pn代表该取值对应的属于类别n的概率。
<p>3.2加权平均属性A的所有取值的信息熵，EA=∑权重（A属性取该值的概率）*该值的EAV
<p>4.求信息增益：EAZ=ES-EA；在所有属性中取信息增益最大的作为决策节点，该节点的不同取值作为分支。
<p>5.递归建立分支节点：对于A=V，选择对应的数据子集，剔除A属性，对该数据集采用ID3算法，
<p>6.返回4中建立的头结点。
<p>ID3分类思路：算法描述
<p>输入： ID3决策树，分类样本。
<p>输出：分类标签
<p>1.如果ID头结点是叶子节点，返回分类标签的值。
<p>2.读取头结点属性标签，以及分类样本对应属性值
<p>3.根据对应属性值，查找对应的分支。
<p>4.递归调用ID3分类函数，输入分支树、待分类样本。
<p>5.return 递归返回值。
<p>参考资料：
<p><a href="http://blog.csdn.net/guoqiangma/article/details/7188678">http://blog.csdn.net/guoqiangma/article/details/7188678</a>
<p><a href="http://www.cnblogs.com/zhangchaoyang/articles/2196631.html">http://www.cnblogs.com/zhangchaoyang/articles/2196631.html</a>
<p><a href="http://czhsuccess.iteye.com/blog/1864652">http://czhsuccess.iteye.com/blog/1864652</a>
<p>机器学习十大算法（1）C4.5
<p>一、原理
<p>对ID3算法的改进：
<p>1.用信息增益率来选择属性；熵的变化值：把每个属性看成是类别，用信息熵公式计算信息熵HV（同ID3中的第二步），然后用ID3中的信息增益除以HV，得到信息增益率。和ID3中第3步方法不同，不考虑属性值对应类别的熵，只是属性一个维度的熵。
<p>2.决策树构造过程中，进行剪枝，考虑样本少的节点，会产生过拟合问题。
<p>3.可以处理离散型数据。
<p>4.对不完整数据进行处理。
<p>连续属性的处理：
<p>在选择某节点的分支属性时，对于离散型属性，按ID3的算法算增益，然后再算增益率。
<p>对于连续型的属性，则需要对其进行离散化处理：
<p>1.将连续属性值从小到大排序：AV1，AV2,…AVN
<p>2.对1序列可以分成N-1个分割点，也就是有N-1种划分成2份的方法。
<p>3.在N-1种分割方法中寻找一种最佳分割：按C4.5信息增益率计算出N-1中分割的信息增益率，取最大的那个作为该属性的分割，和信息增益率。
<p>后剪枝处理：
<p>避免树的高度无节制增长，避免过度拟合数据。
<p>估计剪枝前后的误差，决定是否剪枝。
<p>Pr[ (−)/( (1−)/) &gt;]= c
<p>其中N是实例的数量，f=E/N为观察到的误差率（其中E为N个实例中分类错误的个数），q为真实的误差率，c为置信度（C4.5算法的一个熟人参数，默认值为0.25），z为对应于置信度c的标准差，其值可根据c的设定值通过查正态分布表得到。
<p>通过上公式可以计算出真实误差率q的一个置信区间上限，用此上限为该节点误差率e做一个悲观的估计：
<p><img title="clip_image002" style="border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px" border="0" alt="clip_image002" src="http://wp.hhy/wp-content/uploads/2014/10/clip_image002.png" width="223" height="106">
<p>根据剪枝前后e的值，从而决定是否需要剪枝。
<p>缺失值的处理：
<p>针对数据缺少某些属性值的情况。
<p>策略一：对于赋予该值为该属性常见的值。
<p>根据一个概率分配所有缺少该属性值的样本一个属性值，使得该属性值分布和之前一致。
<p>C4.5生成算法：
<p>输入：数据集DataSet
<p>输出：决策树根节点
<p>1.创建根节点N
<p>2.计算类别的信息熵，同ID3算法：统计类别标签序列以及属性值序列，&lt;属性值，类别&gt;序列的数量，对于属性值缺失的地方，采用策略一或者策略二进行补充。
<p>3.如果信息熵为0，则设置节点为叶子节点，类别为数据集类别。
<p>4.对于每个属性，计算增益率
<p>4.1如果属性是离散属性，则按ID3算法计算增益，按类别信息熵计算方法计算属性信息熵。最终得到增益率
<p>4.2如果属性是连续属性，则将属性值从小到大排序，N个不同的属性值。
<p>4.2.1在N-1个位置分别将属性分成两部分，离散的属性值。
<p>4.2.2按4.1的方法计算信息增益率。
<p>4.2.3取最大的信息增益率和划分位置，作为该属性的信息增益率。
<p>附：
<p>5.在所有属性增益率中选择最大增益率的属性，作为根节点的属性标签。属性值作为分支条件。
<p>6.递归下一个节点：根据每个属性值构建分支节点，属性值满足对应分支条件的数据当中输入踢去根节点属性的数据集。
<p>7.计算每个节点的分类错误，进行剪枝。？？？剪枝后如何处理
<p>8.return 根节点
<p>C4.5分类算法：同ID3
<p>参考资料：
<p><a href="http://blog.csdn.net/xuxurui007/article/details/18045943">http://blog.csdn.net/xuxurui007/article/details/18045943</a>
<p><a href="http://www.cnblogs.com/zhangchaoyang/articles/2842490.html">http://www.cnblogs.com/zhangchaoyang/articles/2842490.html</a>
<p><a href="http://wenku.baidu.com/link?url=G7mc7ywnFgAc62PAIVkSqVHbS3TRYrN0rI-q8tspmgnDFGRngeoKCM_FPL31s9K7G4RoA45fZzeFNyube_LzhZfrkjY8XO41MkiINtyZqfa">http://wenku.baidu.com/link?url=G7mc7ywnFgAc62PAIVkSqVHbS3TRYrN0rI-q8tspmgnDFGRngeoKCM_FPL31s9K7G4RoA45fZzeFNyube_LzhZfrkjY8XO41MkiINtyZqfa</a>
<p>weka.classifiers.trees.j48
<p>机器学习十大算法（10）CART
<p>Classification and regression trees
<p>特点:采用二分递归分割技术，将当前样本分成两个子集，使得生成的每个非叶子节点都有两个分支。二叉树。
<p>针对离散属性进行分类，针对连续属性则进行回归。
<p>数据划分策略：Gini指标
<p><img title="clip_image003" style="border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px" border="0" alt="clip_image003" src="http://wp.hhy/wp-content/uploads/2014/10/clip_image003.gif" width="177" height="49">
<p>计算方式：
<p>对于每一个属性，划分成两部分，每部分数据中，计算Gini指标，然后用加权平均两部分的GinI指标作为该属性的Gini指标。权值为该部分数据所占的比例。
<p>在所有属性中选择属性Gini指标最小的一个作为决策节点，由此数据集被分成较纯的两部分。
<p>关键：根据属性值，划分成两部分，这个真子集比较难选，2^N种可能。
<p>有个问题：如果一个属性中的两个值的区别是区分某两个或两类样本的关键，在该属性的二元划分中没有将其划分开来，那怎么办？
<p>参考资料：
<p><a href="http://blog.csdn.net/hewei0241/article/details/8280490">http://blog.csdn.net/hewei0241/article/details/8280490</a>
<p><a href="http://blog.sina.com.cn/s/blog_7399ad1f01014oic.html">http://blog.sina.com.cn/s/blog_7399ad1f01014oic.html</a></p>
]]></content:encoded>
			<wfw:commentRss>http://wp.hhy/?feed=rss2&#038;p=15</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>数据挖掘测试</title>
		<link>http://wp.hhy/?p=5</link>
		<comments>http://wp.hhy/?p=5#comments</comments>
		<pubDate>Fri, 03 Oct 2014 14:01:23 +0000</pubDate>
		<dc:creator><![CDATA[huahuiyou]]></dc:creator>
				<category><![CDATA[数据挖掘]]></category>

		<guid isPermaLink="false">http://wp.hhy/?p=5</guid>
		<description><![CDATA[测试数据挖掘]]></description>
				<content:encoded><![CDATA[<p>测试数据挖掘</p>
]]></content:encoded>
			<wfw:commentRss>http://wp.hhy/?feed=rss2&#038;p=5</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
	</channel>
</rss>
