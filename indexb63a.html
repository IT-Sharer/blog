<?xml version="1.0" encoding="UTF-8"?><rss version="2.0"
	xmlns:content="http://purl.org/rss/1.0/modules/content/"
	xmlns:wfw="http://wellformedweb.org/CommentAPI/"
	xmlns:dc="http://purl.org/dc/elements/1.1/"
	xmlns:atom="http://www.w3.org/2005/Atom"
	xmlns:sy="http://purl.org/rss/1.0/modules/syndication/"
	xmlns:slash="http://purl.org/rss/1.0/modules/slash/"
	>

<channel>
	<title>IT-Sharer &#187; 数据挖掘</title>
	<atom:link href="http://wp.hhy/?feed=rss2&#038;cat=2" rel="self" type="application/rss+xml" />
	<link>http://wp.hhy</link>
	<description>温 润 醇 和    臻 于 至 善</description>
	<lastBuildDate>Mon, 13 Oct 2014 13:20:55 +0000</lastBuildDate>
	<language>zh-CN</language>
		<sy:updatePeriod>hourly</sy:updatePeriod>
		<sy:updateFrequency>1</sy:updateFrequency>
	<generator>http://wordpress.org/?v=4.0</generator>
	<item>
		<title>近邻原则系列算法</title>
		<link>http://wp.hhy/?p=180</link>
		<comments>http://wp.hhy/?p=180#comments</comments>
		<pubDate>Sat, 11 Oct 2014 16:22:23 +0000</pubDate>
		<dc:creator><![CDATA[huahuiyou]]></dc:creator>
				<category><![CDATA[数据挖掘]]></category>

		<guid isPermaLink="false">http://wp.hhy/?p=180</guid>
		<description><![CDATA[一、概述 机器学习中一般是针对决策规则、概率理论、近邻、集成、增量、图论等原则进<span class="ellipsis">&#8230;</span><div class="read-more"><a href="http://wp.hhy/?p=180">查阅全文 &#8250;</a></div><!-- end of .read-more -->]]></description>
				<content:encoded><![CDATA[<p><span style="font-size:12pt"><strong>一、概述<br />
</strong></span></p>
<p>机器学习中一般是针对决策规则、概率理论、近邻、集成、增量、图论等原则进行算法的设计。本文选取利用近邻原则的相关算法进行总结。
</p>
<p>所谓&#8221;近朱者赤，近墨者黑&#8221;，相近的两个对象一般相似度较大。机器学习十大算法中运用近邻原则的算法主要有：KNN和Kmeans两大类。
</p>
<p><span style="font-size:12pt"><strong>二、KNN算法<br />
</strong></span></p>
<ul style="margin-left: 39pt">
<li>
<div style="text-align: justify"><strong>算法描述<br />
</strong></div>
</li>
</ul>
<p>KNN算法的主要思想是在实例集DTrain中取与待分类样本S最近的K个样本，根据K个样本所属分类，投票决定S的分类及对应的可能性。
</p>
<p style="text-align: center"><img src="http://wp.hhy/wp-content/uploads/2014/10/101114_1622_1.png" alt=""/>
	</p>
<ul style="margin-left: 39pt">
<li>
<div style="text-align: justify"><strong>算法伪代码，用TreeSet实现<br />
</strong></div>
</li>
</ul>
<p style="text-align: center"><img src="http://wp.hhy/wp-content/uploads/2014/10/101114_1622_2.png" alt=""/>
	</p>
<p>分别计算S与实例集DTrain中样本的距离，取最近的K个样本。//一般用小根堆进行。
</p>
<p>分别统计K个近邻中出现的类别及数量。
</p>
<p>取最多的一个类别，输出。
</p>
<ul style="margin-left: 39pt">
<li>
<div style="text-align: justify"><strong>算法缺点<br />
</strong></div>
</li>
</ul>
<p>实例集DTrain的选取很关键，太多冗余、样本不均匀或者噪音将导致分类失效。
</p>
<p>计算量太大，每次分类都需要遍历实例集。
</p>
<p>K值的选定需要人为干预，并且K值影响分类效率。
</p>
<p>实例集的样本必须存储。
</p>
<ul style="margin-left: 39pt">
<li>
<div style="text-align: justify"><strong>改进策略<br />
</strong></div>
</li>
<li>
<div style="text-align: justify"><span style="font-size:10pt">实例集DTrain的生成；<br />
</span></div>
<p style="text-align: justify"><span style="font-size:10pt">等比例抽样法，基于样本空间的抽样法，避免样本不均匀。<br />
</span></p>
<p style="text-align: justify"><span style="font-size:10pt">根据样本的有效性，剔除作用较小的样本。<br />
</span></p>
</li>
<li>
<div style="text-align: justify"><span style="font-size:10pt">距离的计算：样本相似度的计算；<br />
</span></div>
<p style="text-align: justify"><span style="font-size:10pt">欧氏距离；余弦相似度。<br />
</span></p>
<p style="text-align: justify"><span style="font-size:10pt">编辑距离（文本处理）<br />
</span></p>
</li>
<li>
<div style="text-align: justify"><span style="font-size:10pt">K值的选择<br />
</span></div>
<p style="text-align: justify"><span style="font-size:10pt">设定一个K值的初始值，然后以一定步长递增或者递减。选取一个较好的K值。<br />
</span></p>
</li>
<li>
<div style="text-align: justify"><span style="font-size:10pt"><strong>K近邻的搜索；<br />
</strong></span></div>
</li>
</ul>
<p>对样本进行组织整理，分层分群，尽可能将计算压缩到一个较小的样本子空间，避免盲目地与训练集中的每个样本进行比较。
</p>
<p style="text-align: justify; margin-left: 39pt"><span style="font-size:10pt"><strong>快速搜索近邻算法<br />
</strong></span></p>
<p style="margin-left: 21pt">将样本集按近邻关系分解成组，给出每组的质心以及组内样本到该质心的最大距离。这些组又可以分层次结构，组分子组等。最终逐渐深入到一个较小的子组中。减少了计算量，但没有达到减少存储量的目的。
</p>
<p style="text-align: justify; margin-left: 39pt"><span style="font-size:10pt"><strong>剪辑近邻法<br />
</strong></span></p>
<p>利用现有样本集对其自身进行剪辑，将不同类别交界处的样本以适当方式筛选，可以实现既减少样本数又提高正确识别率的双重目的。
</p>
<p>剪辑过程：1将样本分成两个独立的子集D1，D2；2.对D1的每个样本S1，找到其D2中的最近邻SD2，如果它们的分类不一致，则从D1中删除S1；3.最终的D1替代样本集。
</p>
<p style="text-align: justify; margin-left: 39pt"><span style="font-size:10pt"><strong>压缩近邻法<br />
</strong></span></p>
<p>压缩方法：1.初始化两个集合D1=Null，D2=D。在D1中任选一个样本加入D2中。2.依次取D2中的样本，用D1中的样本集对其进行近邻分类，若分类正确则跳过该样本，若分类错误，则将该样本加入到D1中，并且从D2删除该样本。3.重复2，直到D2为空，或者D2中已经没有样本加入到D1中。
</p>
<ul style="margin-left: 39pt">
<li>
<div style="text-align: justify"><span style="font-size:10pt"><strong>K近邻的投票策略<br />
</strong></span></div>
<p style="text-align: justify"><span style="font-size:10pt">距离作为权值进行投票：距离小，权重越大。<br />
</span></p>
<p style="text-align: justify"><span style="font-size:10pt">考虑所有样本中每个分类的比例，比例越大反而在投票中权重越小。<br />
</span></p>
<p style="text-align: justify"><span style="font-size:10pt">考虑近邻的K近邻与该近邻相同类别的比例作为该近邻的权重。<br />
</span></p>
<p style="text-align: justify"><span style="font-size:10pt"><strong>TCMKNN：基于直推信度机的KNN算法；<br />
</strong></span></p>
</li>
</ul>
<p>信度定义T（y）：样本所属分类的K个近邻距离和/样本所属分类外的K个近邻距离和。信度越小，表示样本越满足近邻特性。
</p>
<p>样本属于某个分类的概率P：令样本属于该分类；计算信度TY，统计分类中的所有样本信度大于TY的个数countY，countY/y的样本总数即概率P，概率越大，越可能属于该分类。
</p>
<p>样本-分类概率最大的两个为P1，P2.样本属于P1类，且置信度为P1-P2.置信度越大，说明样本相对于其他分类越有可能属于该分类。
</p>
<p style="text-align: justify; margin-left: 39pt">
 </p>
<p style="margin-left: 21pt"><span style="font-size:12pt"><strong>三、KMeans算法<br />
</strong></span></p>
<ul style="margin-left: 39pt">
<li>
<div style="text-align: justify"><strong>算法描述<br />
</strong></div>
</li>
</ul>
<p>基本思想是将数据划分成若干个聚类，使得类内的相似度最大化，类间的相似度最小化。主要方法是初始随机给定的K个簇中心，按照近邻原则把待分类样本点分到各个簇。然后按平均法重新计算各个簇的质心，作为新的簇中心。经过若干次迭代后，簇中心会收敛。
</p>
<p style="text-align: center"><img src="http://wp.hhy/wp-content/uploads/2014/10/101114_1622_3.png" alt=""/>
	</p>
<ul style="margin-left: 39pt">
<li>
<div style="text-align: justify"><strong>算法伪代码<br />
</strong></div>
</li>
</ul>
<p style="text-align: center"><img src="http://wp.hhy/wp-content/uploads/2014/10/101114_1622_4.png" alt=""/>
	</p>
<p>1.初始化K个簇中心；2.将样本分配到k个簇；3.计算质心作为新的簇中心；4.重复2直到簇中心收敛到一个点。
</p>
<ul style="margin-left: 39pt">
<li>
<div style="text-align: justify"><strong>算法缺点<br />
</strong></div>
</li>
</ul>
<p>要事先确定簇数K；
</p>
<p>对初始聚类中心敏感；
</p>
<p><strong>容易陷入局部最优；<br />
</strong></p>
<p>对&#8221;噪声&#8221;和孤立点敏感：噪声和孤立点，异常点容易影响质心的计算。
</p>
<p>不适于发现非凸面形状的簇或大小差别很大的簇。——非凸面簇往往也是线性不可分的。可以用核函数，将原空间投影到高维空间。
</p>
<ul style="margin-left: 39pt">
<li>
<div style="text-align: justify"><strong>改进策略<br />
</strong></div>
</li>
</ul>
<p><strong>（1）k值确定<br />
</strong></p>
<p><span style="text-decoration:underline">层次聚类：<br />
</span></p>
<p>凝聚法（将每个实例看做一个簇，将距离最近的簇合并成一个簇，并用质心代表该簇，当最最近距离达到某个阈值时停止聚类）；AGNES算法
</p>
<p>分裂法（将所有实例集看成一个簇，簇内实例间最大值是簇的直径，将簇分解直到最大直径小于某个阈值）；DIANA算法
</p>
<p><span style="text-decoration:underline">稳定性方法：<br />
</span></p>
<p>用两次重采样（任取一个节点加入D1，该节点最近邻加入D2，重复此过程）将数据集分成2个子集，这这两个子集做相同的聚类，产生K 聚集。K个聚集的相似度反应了聚类的稳定情况：相似度高，说明K值合理。否则需要增大K。
</p>
<p><span style="text-decoration:underline">系统演化方法：<br />
</span></p>
<p>定义稳定状态的条件，例如聚类最大直径，聚类最小数量，最大数量，聚类实例数量限制等等。
</p>
<p>定义不稳定状态：
</p>
<p>将数据集视为伪热力学问题。从K=1出发，不断的分裂合并不满足条件的聚集，最终达到稳定状态。
</p>
<p><span style="text-decoration:underline">Canopy算法初始划分：<br />
</span></p>
<p>将聚类过程分为两个阶段：
</p>
<p>预处理阶段：初始一个实例，把所有相似的节点（满足一定距离条件）放到一个子集Canopy中，再从没有选中过的实例集中选取一个实例构造下一个canopy，直到所有实例至少被选中一次；这样一个实例可以属于多个canopy，canopy之间可以有重叠；
</p>
<p>传统聚类阶段：在每个canopy中运用传统的聚类方式进行聚类。
</p>
<p><strong>（2）初始质心选取<br />
</strong></p>
<p>随机选取法：
</p>
<p>多次运行，每次选取不同的随机初始质心，去有最小SSE（误差的平方和）的簇集。
</p>
<p>采用层次聚类预处理数据集，取簇集中心为初始质心。
</p>
<p>随机选择一个点或选择所有数据集的质心作为初始点，然后选择距离该点最远的点作为第二个点，依次选择K个点。
</p>
<p>Canopy算法第一阶段得到的每个canopy的质心作为初始质心。
</p>
<p><strong>（3）距离度量<br />
</strong></p>
<p>欧几里得距离和余弦相似度。
</p>
<p>欧氏距离需要标准化，距离越大差异越大。
</p>
<p>余弦相似度不需要标准化，值在[-1,1]，值越大差异越小。
</p>
<p><strong>（4）质心计算<br />
</strong></p>
<p>聚簇中所有样本的向量均值。
</p>
<p><strong>（5）算法停止条件<br />
</strong></p>
<p>目标函数达到最优停止：欧式距离采用聚集的平方误差和（实例与质心的距离平方和）最小化；余弦相似度采用聚集余弦相似度和（到质心的余弦相似度）最大化。
</p>
<p><strong>（6）空聚簇的处理<br />
</strong></p>
<p>方法一：选择一个距离当前任何质心最远的点作为空聚簇的质心。这个点在平方误差和中贡献最大。
</p>
<p>方法二：从平方误差和最大的那个簇，选择距离该簇质心最远的点，作为空聚簇的质心。
</p>
<p><strong>（7）EM算法（搜索本站）：<br />
</strong></p>
<p>实例可以属于多个聚簇，突破了聚簇间的差异要求，只要求聚簇内部达到一定的相似性。
</p>
<p style="margin-left: 21pt"><span style="font-size:12pt"><strong>四、运用案例<br />
</strong></span></p>
<p style="margin-left: 21pt">将Kmeans运用在Knn的近邻搜索中，用聚簇中心索引数据集。
</p>
<p style="margin-left: 21pt">
 </p>
<p style="margin-left: 21pt"><span style="font-size:12pt"><strong>五、参考资料<br />
</strong></span></p>
<p style="margin-left: 21pt"><a href="http://wenku.baidu.com/link?url=9AY6aT-NhPMR_f3eKaKZLgTwBqmDv9vL8Drtnnb1j8uMIKQfUd7p794XXnixAZxU4QIYIQBX7qYNwFvCnJwmttJ8J3jfbRC_XvsKlPJ828e">http://wenku.baidu.com/link?url=9AY6aT-NhPMR_f3eKaKZLgTwBqmDv9vL8Drtnnb1j8uMIKQfUd7p794XXnixAZxU4QIYIQBX7qYNwFvCnJwmttJ8J3jfbRC_XvsKlPJ828e</a> KNN
</p>
<p style="margin-left: 21pt">star :<a href="http://wenku.baidu.com/link?url=Ly_YnXItPt689ZDHOkr6jnQsZwIuRrcO3BUBGdrp1996F3AP4wWdwu0CbPnYDptrrnbtChpXL7vNRY3XA5Ul2fHQ_DcEsDQPd8C04rnlYK3">http://wenku.baidu.com/link?url=Ly_YnXItPt689ZDHOkr6jnQsZwIuRrcO3BUBGdrp1996F3AP4wWdwu0CbPnYDptrrnbtChpXL7vNRY3XA5Ul2fHQ_DcEsDQPd8C04rnlYK3</a> KNN
</p>
<p style="margin-left: 21pt"><a href="http://blog.csdn.net/qll125596718/article/details/8243404">http://blog.csdn.net/qll125596718/article/details/8243404</a>  Kmeans
</p>
<p style="margin-left: 21pt"><a href="http://blog.sina.com.cn/s/blog_69c3ea2b0100nitu.html">http://blog.sina.com.cn/s/blog_69c3ea2b0100nitu.html</a>  层次聚类算法
</p>
<p style="margin-left: 21pt"><a href="http://my.oschina.net/liangtee/blog/125407">http://my.oschina.net/liangtee/blog/125407</a> Canopy算法</p>
]]></content:encoded>
			<wfw:commentRss>http://wp.hhy/?feed=rss2&#038;p=180</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>主成分分析-PCA</title>
		<link>http://wp.hhy/?p=146</link>
		<comments>http://wp.hhy/?p=146#comments</comments>
		<pubDate>Fri, 10 Oct 2014 13:50:10 +0000</pubDate>
		<dc:creator><![CDATA[huahuiyou]]></dc:creator>
				<category><![CDATA[数据挖掘]]></category>
		<category><![CDATA[数据处理]]></category>

		<guid isPermaLink="false">http://wp.hhy/?p=146</guid>
		<description><![CDATA[PCA PCA的出发点 PCA是起源于统计学的方法，主要目的是减少统计特征之间的<span class="ellipsis">&#8230;</span><div class="read-more"><a href="http://wp.hhy/?p=146">查阅全文 &#8250;</a></div><!-- end of .read-more -->]]></description>
				<content:encoded><![CDATA[<ol>
<li>
<div style="text-align: justify;"><span style="font-size: 10pt;">PCA<br />
</span></div>
<ol>
<li>
<div style="text-align: justify;"><span style="font-size: 10pt;">PCA的出发点<br />
</span></div>
<p style="text-align: justify;"><span style="font-size: 10pt;">PCA是起源于统计学的方法，主要目的是减少统计特征之间的相关性。<br />
</span></p>
<p style="text-align: justify;"><span style="font-size: 10pt;">①样本数据的特征之间存在线性相关性。<br />
</span></p>
<p style="text-align: justify;"><span style="font-size: 10pt;">        X4=X1+3X2-4X3；X4特征便是冗余特征。<br />
</span></p>
<p style="text-align: justify;"><span style="font-size: 10pt;">②特征维度越高，越耗费资源和时间。处理效率不高，尤其是稀疏矩阵。<br />
</span></p>
<p style="text-align: justify;"><span style="font-size: 10pt;">③二八定律，20%的特征提供80%的学习效果。<br />
</span></p>
<p style="text-align: justify;"><span style="font-size: 10pt;">④维度越多，噪音越大，产生较大的学习干扰。<br />
</span></p>
<p style="text-align: justify;">
</li>
<li>
<div style="text-align: justify;"><span style="font-size: 10pt;">PCA的问题描述<br />
</span></div>
<p style="text-align: justify;"><span style="font-size: 10pt;">求解一个合适的线性无关正交组合<br />
</span></p>
<p style="text-align: justify;"><span style="font-size: 10pt;">                    F(F1，F2，…,Fk)<br />
</span></p>
<p style="text-align: justify;"><span style="font-size: 10pt;">代表原来的线性组合<br />
</span></p>
<p style="text-align: justify;"><span style="font-size: 10pt;">                    X(X1，X2，…，Xn)，<br />
</span></p>
<p style="text-align: justify;"><span style="font-size: 10pt;">其中k&lt;=n。<br />
</span></p>
<p style="text-align: justify;"><span style="font-size: 10pt;">即：求解一个Q(n×k)将X变换为F。<br />
</span></p>
<p style="text-align: justify;"><span style="font-size: 10pt;">            F(F1,F2,…,Fk)=X(X1,X2,…,Xn)Q(n×k)<br />
</span></p>
<p style="text-align: justify;">
</li>
<li>
<div style="text-align: justify;"><span style="font-size: 10pt;">PCA的原理<br />
</span></div>
</li>
</ol>
</li>
</ol>
<p><strong>最大方差理论</strong>——主成分表示的信息量应该是最大化的</p>
<p>在信号处理中认为信号具有较大的方差，噪声有较小的方差，信噪比就是信号与噪声的方差比，越大越好。</p>
<p>最好的k维特征是将n维样本点转换为k维后，每一维上的样本方差都很大。</p>
<p>例子：图1，图2</p>
<p style="text-align: center;"><img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA1.png" alt="" /></p>
<p style="text-align: center;">图1</p>
<p style="text-align: center;"><img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA2.jpg" alt="" /></p>
<p style="text-align: center;">图2</p>
<p>图1显示的是5个样本点，已经做过预处理，均值为0，特征方差归一。图2将它们分别投影到了两个维度。左图投影后样本点的方差较大，因此保留的信息较为充分。</p>
<p>假设我们要投影的特征的方向向量为u，且是一个单位向量。并且这些实例经过中心化处理，每一维特征均值都为0。因此投影到u上的样本点（只有一个到原点的距离值）的均值仍然是0。</p>
<p>因此投影后的方差可以表示为</p>
<p style="text-align: center;"><img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA3.png" alt="" /></p>
<p><a name="OLE_LINK5"></a>用<a href="http://images.cnblogs.com/cnblogs_com/jerrylead/201104/201104182110582116.png"><img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA4.png" alt="" border="0" /></a>来表示<a href="http://images.cnblogs.com/cnblogs_com/jerrylead/201104/201104182110592606.png"><img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA5.png" alt="" border="0" /></a>，<a href="http://images.cnblogs.com/cnblogs_com/jerrylead/201104/201104182110591145.png"><img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA6.png" alt="" border="0" /></a>表示<a href="http://images.cnblogs.com/cnblogs_com/jerrylead/201104/201104182111006095.png"><img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA7.png" alt="" border="0" /></a>，那么上式写作</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/jerrylead/201104/201104182111009443.png"><img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA8.png" alt="" border="0" /></a><em><br />
</em></p>
<p><a name="OLE_LINK7"></a>由于u是单位向量，即<a href="http://images.cnblogs.com/cnblogs_com/jerrylead/201104/201104182111004393.png"><img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA9.png" alt="" border="0" /></a>，上式两边都左乘u得，<a href="http://images.cnblogs.com/cnblogs_com/jerrylead/201104/201104182111017392.png"><img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA10.png" alt="" border="0" /></a></p>
<p>即<a href="http://images.cnblogs.com/cnblogs_com/jerrylead/201104/201104182111018754.png"><img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA11.png" alt="" border="0" /></a></p>
<p>也就是说，<a href="http://images.cnblogs.com/cnblogs_com/jerrylead/201104/201104182111026180.png"><img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA12.png" alt="" border="0" /></a>就是<a href="http://images.cnblogs.com/cnblogs_com/jerrylead/201104/20110418211103782.png"><img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA13.png" alt="" border="0" /></a>的特征值，u是特征向量。最佳的投影直线是特征值<a href="http://images.cnblogs.com/cnblogs_com/jerrylead/201104/201104182111038523.png"><img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA14.png" alt="" border="0" /></a>最大时对应的特征向量。</p>
<p><a name="OLE_LINK9"></a>因此，我们只需要对协方差矩阵进行特征值分解，得到的前k大特征值对应的特征向量就是最佳的k维新特征，而且这k维新特征是正交的。得到前k个u以后，实例<a href="http://images.cnblogs.com/cnblogs_com/jerrylead/201104/201104182111052502.png"><img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA15.png" alt="" border="0" /></a>通过以下变换可以得到新的样本。</p>
<p><a href="http://images.cnblogs.com/cnblogs_com/jerrylead/201104/201104182111054945.png"><img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA16.png" alt="" border="0" /></a></p>
<p>其中的第j维就是<a href="http://images.cnblogs.com/cnblogs_com/jerrylead/201104/201104182111063799.png"><img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA17.png" alt="" border="0" /></a>在<a href="http://images.cnblogs.com/cnblogs_com/jerrylead/201104/201104182111074812.png"><img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA18.png" alt="" border="0" /></a>上的投影。</p>
<p>通过选取最大的k个u，使得方差较小的特征（如噪声）被丢弃。</p>
<ol>
<li>
<div style="text-align: justify;"><span style="font-size: 10pt;">PCA算法过程<br />
</span></div>
</li>
</ol>
<p><span style="font-size: 12pt;"><span style="font-family: 宋体;"><a name="OLE_LINK12"></a>将所获得的<img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA19.png" alt="" />个指标(每一指标有<img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA20.png" alt="" />个样品）的一批数据写成一个(<img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA21.png" alt="" />)维数据矩阵<img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA22.png" alt="" />．</span><br />
</span></p>
<p><span style="font-size: 12pt;"><span style="font-family: 宋体;">1、对矩阵</span><em>A</em><span style="font-family: 宋体;">作标准化处理：即对每一个指标分量进行标准化处理。<br />
</span></span></p>
<p style="text-align: center;"><img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA23.png" alt="" /><span style="font-family: 宋体; font-size: 12pt;">或者<br />
</span></p>
<p style="text-align: center;"><span style="font-family: 宋体; font-size: 12pt;">其中样本均值： <img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA24.png" alt="" /><br />
</span></p>
<p style="text-align: center;"><span style="font-family: 宋体; font-size: 12pt;">样本标准差： <img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA25.png" alt="" /><br />
</span></p>
<p><span style="font-family: 宋体; font-size: 12pt;">从而得到<img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA26.png" alt="" />。<br />
</span></p>
<p><span style="font-family: 宋体; font-size: 12pt;"><a name="OLE_LINK16"></a>2、计算样本矩阵的<strong>相关系数矩阵</strong><br />
</span></p>
<p style="text-align: center;"><img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA27.png" alt="" /><span style="font-family: 宋体; font-size: 12pt;"><br />
</span></p>
<p><span style="font-size: 12pt;"><span style="font-family: 宋体;">3、计算</span><em>R</em><span style="font-family: 宋体;">的特征值<img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA28.png" alt="" />，即对应的特征向量<img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA29.png" alt="" />。<br />
</span></span></p>
<p><span style="font-family: 宋体; font-size: 12pt;">4、特征值按降序排序(通过选择排序)得<img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA30.png" alt="" />并对特征向量进行相应调整得<img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA31.png" alt="" />。<br />
</span></p>
<p><span style="font-family: 宋体; font-size: 12pt;">5、通过<strong>施密特正交化方法单位正交化</strong>特征向量，得到<img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA32.png" alt="" />。<br />
</span></p>
<p><span style="font-family: 宋体; font-size: 12pt;">6、计算特征值的累积贡献率<img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA33.png" alt="" />，根据给定的提取效率<img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA34.png" alt="" />和限定的特征数量K,如果<img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA35.png" alt="" />||t=K,则提取<img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA36.png" alt="" />个主成分<img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA37.png" alt="" />。<br />
</span></p>
<p><span style="font-family: 宋体; font-size: 12pt;">7、计算已标准化的样本数据X在提取出的特征向量上的投影<img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA38.png" alt="" />，其中<img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA39.png" alt="" />。<br />
</span></p>
<ol>
<li>
<div style="text-align: justify;"><span style="font-size: 10pt;">PCA实例分析<br />
</span></div>
</li>
</ol>
<p>假设我们得到的2维数据如下：</p>
<p style="margin-left: 21pt;">     <a href="http://images.cnblogs.com/cnblogs_com/jerrylead/201104/201104182110393017.png"><img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA40.png" alt="" border="0" /></a></p>
<p style="margin-left: 21pt;">     行代表了实例，列代表特征，这里有10个实例，每个实例两个特征。</p>
<p style="margin-left: 21pt;">     <strong>第1步</strong>分别求x和y的平均值，然后对于所有的实例，都减去对应的均值。这里x的均值是1.81，y的均值是1.91，那么一个实例减去均值后即为（0.69,0.49），得到</p>
<p style="margin-left: 21pt;">     <a href="http://images.cnblogs.com/cnblogs_com/jerrylead/201104/201104182110402112.png"><img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA41.png" alt="" border="0" /></a></p>
<p style="margin-left: 21pt;">     <strong>第2步</strong>，求特征协方差矩阵，如果数据是3维，那么协方差矩阵是</p>
<p style="margin-left: 21pt;">     <a href="http://images.cnblogs.com/cnblogs_com/jerrylead/201104/201104182110404031.png"><img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA42.png" alt="" border="0" /></a></p>
<p style="margin-left: 21pt;">     这里只有x和y，求解得</p>
<p style="margin-left: 21pt;">     <a href="http://images.cnblogs.com/cnblogs_com/jerrylead/201104/201104182110417586.png"><img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA43.png" alt="" border="0" /></a></p>
<p style="margin-left: 21pt;">     对角线上分别是x和y的方差，非对角线上是协方差。协方差大于0表示x和y若有一个增，另一个也增；小于0表示一个增，一个减；协方差为0时，两者独立。协方差绝对值越大，两者对彼此的影响越大，反之越小。</p>
<p style="margin-left: 21pt;">     <strong>第3步</strong>，求协方差的特征值和特征向量，得到</p>
<p style="margin-left: 21pt;">     <a href="http://images.cnblogs.com/cnblogs_com/jerrylead/201104/201104182110413965.png"><img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA44.png" alt="" border="0" /></a></p>
<p style="margin-left: 21pt;">     上面是两个特征值，下面是对应的特征向量，特征值0.0490833989对应特征向量为<a href="http://images.cnblogs.com/cnblogs_com/jerrylead/201104/201104182110418392.png"><img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA45.png" alt="" border="0" /></a>，这里的特征向量都归一化为单位向量。</p>
<p style="margin-left: 21pt;">    <strong>第4步</strong>，将特征值按照从大到小的顺序排序，选择其中最大的k个，然后将其对应的k个特征向量分别作为列向量组成特征向量矩阵。</p>
<p style="margin-left: 21pt;">     这里特征值只有两个，我们选择其中最大的那个，这里是1.28402771，对应的特征向量是<a href="http://images.cnblogs.com/cnblogs_com/jerrylead/201104/201104182110412504.png"><img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA46.png" alt="" border="0" /></a>。</p>
<p style="margin-left: 21pt;">     <strong>第5步</strong>，将样本点投影到选取的特征向量上。假设实例数为m，特征数为n，减去均值后的样本矩阵为DataAdjust(m*n)，协方差矩阵是n*n，选取的k个特征向量组成的矩阵为EigenVectors(n*k)。那么投影后的数据FinalData为</p>
<p style="margin-left: 21pt;">     <a href="http://images.cnblogs.com/cnblogs_com/jerrylead/201104/201104182110424979.png"><img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA47.png" alt="" border="0" /></a></p>
<p style="margin-left: 21pt;">     这里是</p>
<p style="margin-left: 21pt;">     FinalData(10*1) = DataAdjust(10*2矩阵)×特征向量<a href="http://images.cnblogs.com/cnblogs_com/jerrylead/201104/201104182110425818.png"><img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA48.png" alt="" border="0" /></a></p>
<p style="margin-left: 21pt;">     得到结果是</p>
<p style="margin-left: 21pt;">     <a href="http://images.cnblogs.com/cnblogs_com/jerrylead/201104/201104182110427737.png"><img src="http://wp.hhy/wp-content/uploads/2014/10/101014_1349_PCA49.png" alt="" border="0" /></a></p>
<p style="margin-left: 21pt;">     这样，就将原始实例的n维特征变成了k维，这k维就是原始特征在k维上的投影。</p>
<p>&nbsp;</p>
<ol>
<li>
<div style="text-align: justify;"><span style="font-size: 10pt;">代码解读<br />
</span></div>
<p style="text-align: justify;"><span style="font-size: 10pt;"><strong>Weka包说明<br />
</strong></span></p>
<p style="text-align: justify;"><span style="font-size: 10pt;">开源的数据挖掘工具包，有可视化操作界面，也提供JAVA的jar和C#的动态链接库。<br />
</span></p>
<p style="text-align: justify;"><span style="font-size: 10pt;">主要功能包括分类、聚类、关联规则、属性选择、训练集选择等核心功能。<br />
</span></p>
<p style="text-align: justify;"><span style="font-size: 10pt;">同时提供了多种的数据装载、筛选、清洗、转换工具；<br />
</span></p>
<p style="text-align: justify;"><span style="font-size: 10pt;">提供了强大的可视化组件。<br />
</span></p>
<p style="text-align: justify;"><span style="font-size: 10pt;">提供了知识挖掘的工作流可视化开发环境。<br />
</span></p>
<p style="text-align: justify;"><span style="font-size: 10pt;"><strong>PrincipalComponentAnalysis类说明<br />
</strong></span></p>
<p style="text-align: justify;"><span style="font-size: 10pt;">有两个版本的类型实现。继承Filters类的是最新版的。在原版基础上修改而来。<br />
</span></p>
<p style="text-align: justify;"><span style="font-size: 10pt;"><strong>具体方法说明——根据PCA过程，一个一个指定具体的行或者函数。<br />
</strong></span></p>
<p style="text-align: justify;"><span style="font-size: 10pt;">输入实例instances()：<br />
</span></p>
<p style="text-align: justify;"><span style="font-size: 10pt;">输出结果：<br />
</span></p>
<p style="text-align: justify;"><span style="font-size: 10pt;">可以设置的参数：<br />
</span></p>
<p style="text-align: justify;"><span style="font-size: 10pt;">PCA过程描述：<br />
</span></p>
<ol>
<li>
<div style="text-align: justify;"><span style="font-size: 10pt;">标准化：<br />
</span></div>
</li>
<li>
<div style="text-align: justify;"><span style="font-size: 10pt;">求协方差矩阵C：<br />
</span></div>
</li>
<li>
<div style="text-align: justify;"><span style="font-size: 10pt;">求特征值和特征向量：<br />
</span></div>
</li>
<li>
<div style="text-align: justify;"><span style="font-size: 10pt;">排序并筛选：<br />
</span></div>
</li>
<li>
<div style="text-align: justify;"><span style="font-size: 10pt;">正交化，转换为单位向量：<br />
</span></div>
</li>
<li>
<div style="text-align: justify;"><span style="font-size: 10pt;">实例转换并输出<br />
</span></div>
</li>
</ol>
<p style="text-align: justify;"><span style="font-size: 10pt;"><strong>使用说明<br />
</strong></span></p>
</li>
</ol>
<p><span style="color: black; font-family: Courier New; font-size: 10pt;"><a name="OLE_LINK18"></a>FileReader reader=<span style="color: #7f0055;"><strong>new</strong><span style="color: black;"> FileReader(fileString);</span><br />
</span></span></p>
<p><span style="color: black; font-family: Courier New; font-size: 10pt;">            Instances <span style="background-color: yellow;">instances</span>=<span style="color: #7f0055;"><strong>new</strong><span style="color: black;"> Instances(reader);</span><br />
</span></span></p>
<p><span style="color: black; font-family: Courier New; font-size: 10pt;"><strong>            PrincipalComponents pca=<span style="color: #7f0055;">new<span style="color: black;"> PrincipalComponents();</span><br />
</span></strong></span></p>
<p><span style="font-family: Courier New; font-size: 10pt;"><strong><span style="color: black;">            pca.setInputFormat(<span style="background-color: lightgrey;">instances</span>);</span><br />
</strong></span></p>
<p><span style="font-family: Courier New; font-size: 10pt;"><strong><span style="color: black;">            pca.setMaximumAttributes(5);</span><br />
</strong></span></p>
<p><span style="font-family: Courier New; font-size: 10pt;"><strong><span style="color: black;">            Instances instances2=Filter.<em>useFilter</em>(<span style="background-color: lightgrey;">instances</span>, pca);</span><br />
</strong></span></p>
<p><span style="color: black; font-size: 10pt;"><span style="font-family: Courier New;">            System.<span style="color: #0000c0;"><em>out</em><span style="color: black;">.println(<span style="color: #2a00ff;">&#8220;PCA</span></span></span></span><span style="font-family: 宋体;">前数据样本：</span><span style="color: #2a00ff; font-family: Courier New;">&#8220;<span style="color: black;">);</span><br />
</span></span></p>
<p><span style="color: black; font-family: Courier New; font-size: 10pt;">            <span style="color: #7f0055;"><strong>for</strong><span style="color: black;">(<span style="color: #7f0055;"><strong>int</strong><span style="color: black;"> i=0;i&lt;5;i++) System.<span style="color: #0000c0;"><em>out</em><span style="color: black;">.println(<span style="background-color: lightgrey;">instances</span>.instance(i));</span><br />
</span></span></span></span></span></span></p>
<p><span style="color: black; font-size: 10pt;"><span style="font-family: Courier New;">            System.<span style="color: #0000c0;"><em>out</em><span style="color: black;">.println(<span style="color: #2a00ff;">&#8220;PCA </span></span></span></span><span style="font-family: 宋体;">后数据样本：</span><span style="color: #2a00ff; font-family: Courier New;">&#8220;<span style="color: black;">);</span><br />
</span></span></p>
<p><span style="color: black; font-family: Courier New; font-size: 10pt;">            <span style="color: #7f0055;"><strong>for</strong><span style="color: black;">(<span style="color: #7f0055;"><strong>int</strong><span style="color: black;"> i=0;i&lt;5;i++) System.<span style="color: #0000c0;"><em>out</em><span style="color: black;">.println(instances2.instance(i));</span><br />
</span></span></span></span></span></span></p>
<p>特点，基于数据的统计规律，没有太多的参数。处理过程比较固定。</p>
]]></content:encoded>
			<wfw:commentRss>http://wp.hhy/?feed=rss2&#038;p=146</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>数据挖掘原语-形式化定义过程描述</title>
		<link>http://wp.hhy/?p=77</link>
		<comments>http://wp.hhy/?p=77#comments</comments>
		<pubDate>Thu, 09 Oct 2014 09:21:06 +0000</pubDate>
		<dc:creator><![CDATA[huahuiyou]]></dc:creator>
				<category><![CDATA[数据挖掘]]></category>

		<guid isPermaLink="false">http://wp.hhy/?p=77</guid>
		<description><![CDATA[数据挖掘即是一个需要人工干预和指导，又是一个高度流程化的一个过程。这就使得在数据<span class="ellipsis">&#8230;</span><div class="read-more"><a href="http://wp.hhy/?p=77">查阅全文 &#8250;</a></div><!-- end of .read-more -->]]></description>
				<content:encoded><![CDATA[<p>数据挖掘即是一个需要人工干预和指导，又是一个高度流程化的一个过程。这就使得在数据挖掘系统设计中需要考虑用用于交互的模块，也要将流程化的模块固定下来。
</p>
<p>数据挖掘挖掘原语主要解决的就是上述问题。原语不仅定义了数据挖掘的要素和主要流程，还定义了每个流程和要素模块，用户是如何介入的。
</p>
<p>数据挖掘原语组成：任务相关的数据，要挖掘的知识类型，指导挖掘过程的背景知识，模式评估兴趣度量和显示所发现的知识。
</p>
<ol>
<li>
<div style="text-align: justify"><span style="font-size:10pt">任务相关的数据：定义数据输入的来源，格式。常常由用户选定数据集。<br />
</span></div>
</li>
<li>
<div style="text-align: justify"><span style="font-size:10pt">要挖掘的知识类型：描述和区分，分类和预测，聚类，异常点检测，时序模型等等。通常知识类型描述的是需要的输出结果，例如一个统计数据的方法、分类器，异常检测器等等。可以利用这些算法（机）输出知识信息。<br />
</span></div>
</li>
<li>
<div style="text-align: justify"><span style="font-size:10pt">指导的背景知识：已有的一些知识。例如挖掘对象的语义模型，概念分层模型。属性间的关联规则等等。<br />
</span></div>
</li>
<li>
<div style="text-align: justify"><span style="font-size:10pt">兴趣度量：对挖掘结果的评估和筛选。所有数据挖掘结果并不一定是有用的，合理的。<br />
</span></div>
</li>
<li>
<div style="text-align: justify"><span style="font-size:10pt">知识可视化：图表，多维度展示，规则库等等方式。<br />
</span></div>
<p style="margin-left: 21pt">
 </p>
<p style="margin-left: 21pt"><strong>相关产品：<br />
</strong></p>
<p>SQLSERVER Analysis服务系列：可以从数据仓库中选择一个数据立方或者多个数据集，然后选择指定的算法进行挖掘。挖掘结果可以用决策树，图表等多种展现方式。
</p>
<p>Weka平台：支持数据的导入、清洗，数据挖掘方法的选择，结果可视化等。
</p>
<p>
 </p>
<p><strong>研究点：</strong>数据挖掘查询语言DMQL
</p>
<p>目前对用户指导的背景知识设计这块还比较欠缺。大多数产品都是选择模式进行数据的处理。而没有采取融合多个算法模式进行挖掘。
</p>
<p>背景知识利用的不充分。用户可以自定义的背景知识较为有限。
</p>
<p><img src="http://wp.hhy/wp-content/uploads/2014/10/100914_0921_1.png" alt=""/></p>
</li>
</ol>
]]></content:encoded>
			<wfw:commentRss>http://wp.hhy/?feed=rss2&#038;p=77</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>数据预处理概述</title>
		<link>http://wp.hhy/?p=46</link>
		<comments>http://wp.hhy/?p=46#comments</comments>
		<pubDate>Tue, 07 Oct 2014 09:07:07 +0000</pubDate>
		<dc:creator><![CDATA[huahuiyou]]></dc:creator>
				<category><![CDATA[数据挖掘]]></category>

		<guid isPermaLink="false">http://wp.hhy/?p=46</guid>
		<description><![CDATA[在数据挖掘整个项目中，数据准备（采集，预处理）可能包含了整个项目进度的20%左右<span class="ellipsis">&#8230;</span><div class="read-more"><a href="http://wp.hhy/?p=46">查阅全文 &#8250;</a></div><!-- end of .read-more -->]]></description>
				<content:encoded><![CDATA[<p>在数据挖掘整个项目中，数据准备（采集，预处理）可能包含了整个项目进度的20%左右，主要问题是清洗、转换、集成、和归约。</p>
<p><strong>1.数据预处理的目的，对应方法</strong></p>
<p>对不完整的，含噪音的和不一致的数据进行处理；====》数据清理</p>
<p>对数据进行转换，以满足数据挖掘算法的需求；=====》数据转换</p>
<p>数据集成：将多源异构的数据集成到一起，增加样本多样性；===》数据集成</p>
<p>聚集、删除冗余特征，压缩数据。====》数据归约</p>
<p><strong>2.数据清理</strong></p>
<p><strong>2.1</strong>缺失值处理</p>
<ul>
<ul>
<li>忽略该样本：多个主要属性缺失，该实例为无效实例；</li>
<li>人工填写遗漏值：少量缺失，可以人工填充；</li>
<li>使用默认值填充：设置对应属性的默认值；</li>
<li>使用属性的平均值；</li>
<li>使用给定样本所属类别中的该属性的平均值；</li>
<li>时间序列样本，采用平滑拟合后该时间点出的值；</li>
</ul>
</ul>
<p>2.2噪音数据处理</p>
<blockquote><p>分箱：考察邻居的值，平滑数据。</p>
<p>将数据划分成若干小的箱子，在箱子内部可以采用按平均值、中值、边界平滑等策略</p>
<p>聚类：将样本数据按照聚类规则聚类，满足一定相似度条件的数据平滑</p>
<p>人工处理</p>
<p>回归：使用回归函数，平滑数据。</p>
</blockquote>
<p>2.3不一致数据</p>
<p>根据一致性规则，找出不符合规则的样本，进行更正。</p>
<p><strong>3.数据集成和变换</strong></p>
<p>3.1集成：主要考虑异源异构的数据集之间的集成，结构统一问题</p>
<p>3.2变换</p>
<blockquote><p>数据平滑：分箱平滑、聚类和回归；</p>
<p>规范化：最小-最大规范化；Z-Score规范化；</p>
</blockquote>
<p><strong>4.数据归约</strong></p>
<p>4.1属性选择</p>
<blockquote><p>逐步向前选择：从属性集中选择最好的属性，加入待选属性，直到属性不满足条件停止。决策树系列。</p>
<p>逐步向后剔除：从属性集中剔除最坏的属性，可以用信息熵或者信息增益衡量好坏。</p>
<p>向前选择和向后剔除结合：结合前两种方式。</p>
</blockquote>
<p>4.2数据压缩</p>
<blockquote><p>小波变换：使得数据波在一段长度内变得平滑。从而只保留少部分参数就可以保存整段数据。</p>
</blockquote>
<p>4.3主成分分析PCA</p>
<p>求得一个最小的线性无关组合。</p>
<p>4.4数值归约</p>
<p>用一个规则保存一整段的数据，而不是一个一个数据保存。</p>
<p>分箱平滑，聚集，回归等。</p>
<p>4.5数据抽样</p>
<p>不放回抽样，放回抽样，聚类抽样，分层抽样，等比例分类抽样。</p>
<p>4.6离散化与概念分层</p>
<blockquote><p>连续属性的离散化：分箱（等距离散化）、聚类分析、基于信息增益的离散化（C4.5算法中采用）、自然划分（3-4-5规则）</p>
</blockquote>
]]></content:encoded>
			<wfw:commentRss>http://wp.hhy/?feed=rss2&#038;p=46</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>数据挖掘的任务和功能</title>
		<link>http://wp.hhy/?p=42</link>
		<comments>http://wp.hhy/?p=42#comments</comments>
		<pubDate>Tue, 07 Oct 2014 03:35:00 +0000</pubDate>
		<dc:creator><![CDATA[huahuiyou]]></dc:creator>
				<category><![CDATA[数据挖掘]]></category>
		<category><![CDATA[基础]]></category>

		<guid isPermaLink="false">http://wp.hhy/?p=42</guid>
		<description><![CDATA[数据挖掘的任务一般可以分为两类:描述和预测。描述性任务一般刻画数据中的一般特性，<span class="ellipsis">&#8230;</span><div class="read-more"><a href="http://wp.hhy/?p=42">查阅全文 &#8250;</a></div><!-- end of .read-more -->]]></description>
				<content:encoded><![CDATA[<p>数据挖掘的任务一般可以分为两类:描述和预测。描述性任务一般刻画数据中的一般特性，例如统计等；预测性挖掘任务在当前数据上进行推断和预测。</p>
<p>数据挖掘的功能常见如下：</p>
<p>1.概念/类描述:特征和区分</p>
<p>描述数据的特征,汇总统计,多维度展现，概念比较等手段。</p>
<p>2.关联分析</p>
<p>发现数据事务中的频繁模式。有较高的支持度和置信度。</p>
<p>3.分类和预测</p>
<p>对&lt;x,y&gt;的数据建立分类函数F，使得对于xi可以求得y=f(xi)，</p>
<p>其中x是实例的特征表示，y是实例的类标号。</p>
<p>4.聚类分析</p>
<p>实例根据最大化类内相似性、最小化类间相似性的原则进行聚类或分组。</p>
<p>常常也利用聚类的这一原则进行决策支持。</p>
<p>5.异常点检测</p>
<p>发现实例群形成一般化特征以外的异常实例。常常使用偏差法。引入一定偏差变量，例如SVM中的松弛变量。</p>
<p>6.演变分析</p>
<p>描述数据随时间变化的规律或者趋势。</p>
<p>常常是时间序列分析。</p>
<p>也有偏序关系分析：事务关联分析中两个事务具有时间顺序的关系。</p>
]]></content:encoded>
			<wfw:commentRss>http://wp.hhy/?feed=rss2&#038;p=42</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>数据挖掘十大算法</title>
		<link>http://wp.hhy/?p=17</link>
		<comments>http://wp.hhy/?p=17#comments</comments>
		<pubDate>Sat, 04 Oct 2014 05:01:37 +0000</pubDate>
		<dc:creator><![CDATA[huahuiyou]]></dc:creator>
				<category><![CDATA[数据挖掘]]></category>

		<guid isPermaLink="false">http://wp.hhy/?p=17</guid>
		<description><![CDATA[国际权威的学术组织the IEEE International Conferen<span class="ellipsis">&#8230;</span><div class="read-more"><a href="http://wp.hhy/?p=17">查阅全文 &#8250;</a></div><!-- end of .read-more -->]]></description>
				<content:encoded><![CDATA[<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:宋体">国际权威的学术组织</span><span style="font-family:Arial">the IEEE International Conference on Data Mining (ICDM) 2006</span><span style="font-family:宋体">年</span><span style="font-family:Arial">12</span><span style="font-family:宋体">月评选出了数据挖掘领域的十大经典算法：</span><span style="font-family:Arial">C4.5, k-Means, SVM, Apriori, EM, PageRank, AdaBoost, kNN, Naive Bayes, and CART.<br />
</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:宋体">不仅仅是选中的十大算法，其实参加<span style="color:black">评选的</span></span><span style="font-family:Arial">18</span><span style="color:black"><span style="font-family:宋体">种算法，实际上随便拿出一种来都可以称得上是经典算法，它们在数据挖掘领域都产生了极为深远的影响。</span><span style="color:#333333; font-family:Arial"> <br />
</span></span></span></p>
<p style="background: white"><span style="color:black; font-family:Arial; font-size:10pt">1. <a href="http://blog.csdn.net/aladdina/archive/2009/04/30/4141048.aspx" target="_blank"><span style="color:#336699">C4.5</span></a><span style="color:#333333"><br />
			</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">C4.5</span><span style="font-family:宋体">算法是机器学习算法中的一种分类决策树算法</span><span style="font-family:Arial">,</span><span style="font-family:宋体">其核心算法是</span><span style="font-family:Arial">ID3</span><span style="font-family:宋体">算法</span><span style="font-family:Arial">.  C4.5</span><span style="font-family:宋体">算法继承了</span><span style="font-family:Arial">ID3</span><span style="font-family:宋体">算法的优点，并在以下几方面对</span><span style="font-family:Arial">ID3</span><span style="font-family:宋体">算法进行了改进：</span><span style="font-family:Arial"><br />
			</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">1) </span><span style="font-family:宋体">用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足；</span><span style="font-family:Arial"><br/>    2) </span><span style="font-family:宋体">在树构造过程中进行剪枝；</span><span style="font-family:Arial"><br/>    3) </span><span style="font-family:宋体">能够完成对连续属性的离散化处理；</span><span style="font-family:Arial"><br/>    4) </span><span style="font-family:宋体">能够对不完整数据进行处理。</span><span style="font-family:Arial"><br />
			</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">C4.5</span><span style="font-family:宋体">算法有如下优点：产生的分类规则易于理解，准确率较高。其缺点是：在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。</span><span style="font-family:Arial"> <br />
</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">2. <a href="http://blog.csdn.net/aladdina/archive/2009/04/30/4141089.aspx" target="_blank"><span style="color:#336699">The k-means algorithm</span></a> </span><span style="font-family:宋体">即</span><span style="font-family:Arial">K-Means</span><span style="font-family:宋体">算法</span><span style="font-family:Arial"><br />
			</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">k-means algorithm</span><span style="font-family:宋体">算法是一个聚类算法，把</span><span style="font-family:Arial">n</span><span style="font-family:宋体">的对象根据他们的属性分为</span><span style="font-family:Arial">k</span><span style="font-family:宋体">个分割，</span><span style="font-family:Arial">k &lt; n</span><span style="font-family:宋体">。它与处理混合正态分布的最大期望算法很相似，因为他们都试图找到数据中自然聚类的中心。它假设对象属性来自于空间向量，并且目标是使各个群组内部的均</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">方误差总和最小。</span><span style="font-family:Arial"> <br />
</span></span></p>
<p style="background: white"><span style="color:#333333; font-family:Arial; font-size:10pt">3. <a href="http://blog.csdn.net/aladdina/archive/2009/04/30/4141094.aspx" target="_blank"><span style="color:#336699">Support vector machines</span></a><br />
		</span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:宋体">支持向量机，英文为</span><span style="font-family:Arial">Support Vector Machine</span><span style="font-family:宋体">，简称</span><span style="font-family:Arial">SV</span><span style="font-family:宋体">机（论文中一般简称</span><span style="font-family:Arial">SVM</span><span style="font-family:宋体">）。它是一种監督式學習的方法，它广泛的应用于统计分类以及回归分析中。支持向量机将向量映射到一个更</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">高维的空间里，在这个空间里建立有一个最大间隔超平面。在分开数据的超平面的两边建有两个互相平行的超平面。分隔超平面使两个平行超平面的距离最大化。假</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">定平行超平面间的距离或差距越大，分类器的总误差越小。一个极好的指南是</span><span style="font-family:Arial">C.J.C Burges</span><span style="font-family:宋体">的《模式识别支持向量机指南》。</span><span style="font-family:Arial">van der Walt </span><span style="font-family:宋体">和</span><span style="font-family:Arial"> Barnard </span><span style="font-family:宋体">将支持向量机和其他分类器进行了比较。</span><span style="font-family:Arial"> <br />
</span></span></p>
<p style="background: white"><span style="color:#333333; font-family:Arial; font-size:10pt">4. <a href="http://blog.csdn.net/aladdina/archive/2009/05/01/4141101.aspx" target="_blank"><span style="color:#336699">The Apriori algorithm</span></a><br />
		</span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">Apriori</span><span style="font-family:宋体">算法是一种最有影响的挖掘布尔关联规则频繁项集的算法。其核心是基于两阶段频集思想的递推算法。该关联规则在分类上属于单维、单层、布尔关联规则。在这里，所有支持度大于最小支持度的项集称为频繁项集，简称频集。</span><span style="font-family:Arial"> <br />
</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">5. <a href="http://blog.csdn.net/aladdina/archive/2009/05/01/4141114.aspx" target="_blank"/></span><span style="color:#336699"><span style="font-family:宋体">最大期望</span><span style="font-family:Arial">(EM)</span><span style="font-family:宋体">算法</span><span style="color:#333333; font-family:Arial"><br />
				</span></span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:宋体">在统计计算中，最大期望（</span><span style="font-family:Arial">EM</span><span style="font-family:宋体">，</span><span style="font-family:Arial">Expectation–Maximization</span><span style="font-family:宋体">）算法是在概率（</span><span style="font-family:Arial">probabilistic</span><span style="font-family:宋体">）模型中寻找参数最大似然</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">估计的算法，其中概率模型依赖于无法观测的隐藏变量（</span><span style="font-family:Arial">Latent Variabl</span><span style="font-family:宋体">）。最大期望经常用在机器学习和计算机视觉的数据集聚（</span><span style="font-family:Arial">Data Clustering</span><span style="font-family:宋体">）领域。</span><span style="font-family:Arial"> <br />
</span></span></p>
<p style="background: white"><span style="color:#333333; font-family:Arial; font-size:10pt">6. <a href="http://blog.csdn.net/aladdina/archive/2009/05/01/4141120.aspx" target="_blank"><span style="color:#336699">PageRank</span></a><br />
		</span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">PageRank</span><span style="font-family:宋体">是</span><span style="font-family:Arial">Google</span><span style="font-family:宋体">算法的重要内容。</span><span style="font-family:Arial">2001</span><span style="font-family:宋体">年</span><span style="font-family:Arial">9</span><span style="font-family:宋体">月被授予美国专利，专利人是</span><span style="font-family:Arial">Google</span><span style="font-family:宋体">创始人之一拉里</span><span style="font-family:Arial">·</span><span style="font-family:宋体">佩奇（</span><span style="font-family:Arial">Larry Page</span><span style="font-family:宋体">）。因此，</span><span style="font-family:Arial">PageRank</span><span style="font-family:宋体">里的</span><span style="font-family:Arial">page</span><span style="font-family:宋体">不是指网页，而是指佩奇，即这个等级方法是以佩奇来命名的。</span><span style="font-family:Arial"><br />
			</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">PageRank</span><span style="font-family:宋体">根据网站的外部链接和内部链接的数量和质量俩衡量网站的价值。</span><span style="font-family:Arial">PageRank</span><span style="font-family:宋体">背后的概念是，每个到页面的链接都是对该页面的一次投票，</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">被链接的越多，就意味着被其他网站投票越多。这个就是所谓的</span><span style="font-family:Arial">&#8220;</span><span style="font-family:宋体">链接流行度</span><span style="font-family:Arial">&#8220;——</span><span style="font-family:宋体">衡量多少人愿意将他们的网站和你的网站挂钩。</span><span style="font-family:Arial">PageRank</span><span style="font-family:宋体">这个概念引自</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">学术中一篇论文的被引述的频度</span><span style="font-family:Arial">——</span><span style="font-family:宋体">即被别人引述的次数越多，一般判断这篇论文的权威性就越高。</span><span style="font-family:Arial"> <br />
</span></span></p>
<p style="background: white"><span style="color:#333333; font-family:Arial; font-size:10pt">7. <a href="http://blog.csdn.net/aladdina/archive/2009/05/01/4141124.aspx" target="_blank"><span style="color:#336699">AdaBoost</span></a><br />
		</span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">Adaboost</span><span style="font-family:宋体">是一种迭代算法，其核心思想是针对同一个训练集训练不同的分类器</span><span style="font-family:Arial">(</span><span style="font-family:宋体">弱分类器</span><span style="font-family:Arial">)</span><span style="font-family:宋体">，然后把这些弱分类器集合起来，构成一个更强的最终分类器</span><span style="font-family:Arial"> (</span><span style="font-family:宋体">强分类器</span><span style="font-family:Arial">)</span><span style="font-family:宋体">。其算法本身是通过改变数据分布来实现的，它根据每次训练集之中每个样本的分类是否正确，以及上次的总体分类的准确率，来确定每个样本的权</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">值。将修改过权值的新数据集送给下层分类器进行训练，最后将每次训练得到的分类器最后融合起来，作为最后的决策分类器。</span><span style="font-family:Arial"><br />
			</span></span></p>
<p style="background: white"><span style="color:#333333; font-family:Arial; font-size:10pt">8. <a href="http://blog.csdn.net/aladdina/archive/2009/05/01/4141127.aspx" target="_blank"><span style="color:#336699">kNN: k-nearest neighbor classification</span></a><br />
		</span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">K</span><span style="font-family:宋体">最近邻</span><span style="font-family:Arial">(k-Nearest Neighbor</span><span style="font-family:宋体">，</span><span style="font-family:Arial">KNN)</span><span style="font-family:宋体">分类算法，是一个理论上比较成熟的方法，也是最简单的机器学习算法之一。该方法的思路是：如果一个样本在特征空间中的</span><span style="font-family:Arial">k</span><span style="font-family:宋体">个最相似</span><span style="font-family:Arial">(</span><span style="font-family:宋体">即特征空间中最邻近</span><span style="font-family:Arial">)</span><span style="font-family:宋体">的样本中的大多数属于某一个类别，则该样本也属于这个类别。</span><span style="font-family:Arial"><br />
			</span></span></p>
<p style="background: white"><span style="color:#333333; font-family:Arial; font-size:10pt">9. <a href="http://blog.csdn.net/aladdina/archive/2009/05/01/4141140.aspx" target="_blank"><span style="color:#336699">Naive Bayes</span></a><br />
		</span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:宋体">在众多的分类模型中，应用最为广泛的两种分类模型是决策树模型</span><span style="font-family:Arial">(Decision Tree Model)</span><span style="font-family:宋体">和朴素贝叶斯模型（</span><span style="font-family:Arial">Naive Bayesian Model</span><span style="font-family:宋体">，</span><span style="font-family:Arial">NBC</span><span style="font-family:宋体">）。</span><span style="font-family:Arial"><strong> </strong></span><span style="font-family:宋体"><strong>朴素贝叶斯模型</strong>发源于古典数学理论，有着坚实的数学基础，以</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">及稳定的分类效率。同时，</span><span style="font-family:Arial">NBC</span><span style="font-family:宋体">模型所需估计的参数很少，对缺失数据不太敏感，算法也比较简单。理论上，</span><span style="font-family:Arial">NBC</span><span style="font-family:宋体">模型与其他分类方法相比具有最小的误差率。</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">但是实际上并非总是如此，这是因为</span><span style="font-family:Arial">NBC</span><span style="font-family:宋体">模型假设属性之间相互独立，这个假设在实际应用中往往是不成立的，这给</span><span style="font-family:Arial">NBC</span><span style="font-family:宋体">模型的正确分类带来了一定影响。在属</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">性个数比较多或者属性之间相关性较大时，</span><span style="font-family:Arial">NBC</span><span style="font-family:宋体">模型的分类效率比不上决策树模型。而在属性相关性较小时，</span><span style="font-family:Arial">NBC</span><span style="font-family:宋体">模型的性能最为良好。</span><span style="font-family:Arial"> <br />
</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">10. <a href="http://blog.csdn.net/aladdina/archive/2009/05/01/4141150.aspx" target="_blank"><span style="color:#336699">CART: </span></a></span><span style="font-family:宋体">分类与回归树</span><span style="font-family:Arial"><br />
			</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:Arial">CART, Classification and Regression Trees</span><span style="font-family:宋体">。</span><span style="font-family:Arial"><br />
			</span><span style="font-family:宋体">在分类树下面有两个关键的思想。第一个是关于递归地划分自变量空间的想法；第二个想法是用验证数据进行剪枝。</span><span style="font-family:Arial"><br />
			</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:宋体">本文来源：</span><span style="font-family:Arial">http://blog.csdn.net/aladdina/<br />
</span></span></p>
<p style="background: white"><span style="color:#333333; font-size:10pt"><span style="font-family:宋体">上面的</span><span style="font-family:Arial">10</span><span style="font-family:宋体">篇文章的摘要来源全部转载自网络搜索，百度百科内容最多，少量来自中文维基百科以及其他网页。</span><span style="font-family:Arial"><br />
			</span></span></p>
]]></content:encoded>
			<wfw:commentRss>http://wp.hhy/?feed=rss2&#038;p=17</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>决策树系列算法</title>
		<link>http://wp.hhy/?p=15</link>
		<comments>http://wp.hhy/?p=15#comments</comments>
		<pubDate>Sat, 04 Oct 2014 04:49:55 +0000</pubDate>
		<dc:creator><![CDATA[huahuiyou]]></dc:creator>
				<category><![CDATA[数据挖掘]]></category>
		<category><![CDATA[C4.5]]></category>
		<category><![CDATA[CART]]></category>
		<category><![CDATA[ID3]]></category>
		<category><![CDATA[决策树]]></category>

		<guid isPermaLink="false">http://wp.hhy/?p=15</guid>
		<description><![CDATA[ID3算法 ID3算法的核心：用信息增益作为属性选择的标准。 检测所有属性，用信<span class="ellipsis">&#8230;</span><div class="read-more"><a href="http://wp.hhy/?p=15">查阅全文 &#8250;</a></div><!-- end of .read-more -->]]></description>
				<content:encoded><![CDATA[<p>ID3算法
<p>ID3算法的核心：用信息增益作为属性选择的标准。
<p>检测所有属性，用信息增益最大的属性产生决策树的节点，该节点的不同取值建立分支。再对各分支的数据子集调用决策树算法。
<p>直到所有子集仅包含同一类别的数据为止。
<p>缺点：只对小数据集有效，噪声敏感。
<p>信息增益选择属性偏向取值多的属性；
<p>决策树节点的构造：
<p>节点：决策属性名称，决策条件，决策规则，一般采用连续属性离散化去属性值
<p>决策规则判断函数：属于该节点时，往下探查。
<p>ID3算法的思路：算法描述
<p>输入：属性集合，带类别的数据集合
<p>输出决策树：决策树的头结点，见决策树构造
<p>1.从数据集提取类别集合：类别名称，类别数量（每个类别的概率），每个属性的值列表，取值对应每个类别的数量（概率），
<p>2.计算数据集的系统熵：ES，如果ES=0，则该节点为叶子节点，类别为数据集类别
<p><img title="clip_image001" style="border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px" border="0" alt="clip_image001" src="http://wp.hhy/wp-content/uploads/2014/10/clip_image001.png" width="244" height="18">
<p>：用概率乘以概率的对数，再每个类别相减。
<p>3.对于每一个属性A，计算信息熵EA
<p>3.1对于每一个取值AV，计算该取值的信息熵EAV：对应每个类别均有概率，用2方法计算信息熵，Pn代表该取值对应的属于类别n的概率。
<p>3.2加权平均属性A的所有取值的信息熵，EA=∑权重（A属性取该值的概率）*该值的EAV
<p>4.求信息增益：EAZ=ES-EA；在所有属性中取信息增益最大的作为决策节点，该节点的不同取值作为分支。
<p>5.递归建立分支节点：对于A=V，选择对应的数据子集，剔除A属性，对该数据集采用ID3算法，
<p>6.返回4中建立的头结点。
<p>ID3分类思路：算法描述
<p>输入： ID3决策树，分类样本。
<p>输出：分类标签
<p>1.如果ID头结点是叶子节点，返回分类标签的值。
<p>2.读取头结点属性标签，以及分类样本对应属性值
<p>3.根据对应属性值，查找对应的分支。
<p>4.递归调用ID3分类函数，输入分支树、待分类样本。
<p>5.return 递归返回值。
<p>参考资料：
<p><a href="http://blog.csdn.net/guoqiangma/article/details/7188678">http://blog.csdn.net/guoqiangma/article/details/7188678</a>
<p><a href="http://www.cnblogs.com/zhangchaoyang/articles/2196631.html">http://www.cnblogs.com/zhangchaoyang/articles/2196631.html</a>
<p><a href="http://czhsuccess.iteye.com/blog/1864652">http://czhsuccess.iteye.com/blog/1864652</a>
<p>机器学习十大算法（1）C4.5
<p>一、原理
<p>对ID3算法的改进：
<p>1.用信息增益率来选择属性；熵的变化值：把每个属性看成是类别，用信息熵公式计算信息熵HV（同ID3中的第二步），然后用ID3中的信息增益除以HV，得到信息增益率。和ID3中第3步方法不同，不考虑属性值对应类别的熵，只是属性一个维度的熵。
<p>2.决策树构造过程中，进行剪枝，考虑样本少的节点，会产生过拟合问题。
<p>3.可以处理离散型数据。
<p>4.对不完整数据进行处理。
<p>连续属性的处理：
<p>在选择某节点的分支属性时，对于离散型属性，按ID3的算法算增益，然后再算增益率。
<p>对于连续型的属性，则需要对其进行离散化处理：
<p>1.将连续属性值从小到大排序：AV1，AV2,…AVN
<p>2.对1序列可以分成N-1个分割点，也就是有N-1种划分成2份的方法。
<p>3.在N-1种分割方法中寻找一种最佳分割：按C4.5信息增益率计算出N-1中分割的信息增益率，取最大的那个作为该属性的分割，和信息增益率。
<p>后剪枝处理：
<p>避免树的高度无节制增长，避免过度拟合数据。
<p>估计剪枝前后的误差，决定是否剪枝。
<p>Pr[ (−)/( (1−)/) &gt;]= c
<p>其中N是实例的数量，f=E/N为观察到的误差率（其中E为N个实例中分类错误的个数），q为真实的误差率，c为置信度（C4.5算法的一个熟人参数，默认值为0.25），z为对应于置信度c的标准差，其值可根据c的设定值通过查正态分布表得到。
<p>通过上公式可以计算出真实误差率q的一个置信区间上限，用此上限为该节点误差率e做一个悲观的估计：
<p><img title="clip_image002" style="border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px" border="0" alt="clip_image002" src="http://wp.hhy/wp-content/uploads/2014/10/clip_image002.png" width="223" height="106">
<p>根据剪枝前后e的值，从而决定是否需要剪枝。
<p>缺失值的处理：
<p>针对数据缺少某些属性值的情况。
<p>策略一：对于赋予该值为该属性常见的值。
<p>根据一个概率分配所有缺少该属性值的样本一个属性值，使得该属性值分布和之前一致。
<p>C4.5生成算法：
<p>输入：数据集DataSet
<p>输出：决策树根节点
<p>1.创建根节点N
<p>2.计算类别的信息熵，同ID3算法：统计类别标签序列以及属性值序列，&lt;属性值，类别&gt;序列的数量，对于属性值缺失的地方，采用策略一或者策略二进行补充。
<p>3.如果信息熵为0，则设置节点为叶子节点，类别为数据集类别。
<p>4.对于每个属性，计算增益率
<p>4.1如果属性是离散属性，则按ID3算法计算增益，按类别信息熵计算方法计算属性信息熵。最终得到增益率
<p>4.2如果属性是连续属性，则将属性值从小到大排序，N个不同的属性值。
<p>4.2.1在N-1个位置分别将属性分成两部分，离散的属性值。
<p>4.2.2按4.1的方法计算信息增益率。
<p>4.2.3取最大的信息增益率和划分位置，作为该属性的信息增益率。
<p>附：
<p>5.在所有属性增益率中选择最大增益率的属性，作为根节点的属性标签。属性值作为分支条件。
<p>6.递归下一个节点：根据每个属性值构建分支节点，属性值满足对应分支条件的数据当中输入踢去根节点属性的数据集。
<p>7.计算每个节点的分类错误，进行剪枝。？？？剪枝后如何处理
<p>8.return 根节点
<p>C4.5分类算法：同ID3
<p>参考资料：
<p><a href="http://blog.csdn.net/xuxurui007/article/details/18045943">http://blog.csdn.net/xuxurui007/article/details/18045943</a>
<p><a href="http://www.cnblogs.com/zhangchaoyang/articles/2842490.html">http://www.cnblogs.com/zhangchaoyang/articles/2842490.html</a>
<p><a href="http://wenku.baidu.com/link?url=G7mc7ywnFgAc62PAIVkSqVHbS3TRYrN0rI-q8tspmgnDFGRngeoKCM_FPL31s9K7G4RoA45fZzeFNyube_LzhZfrkjY8XO41MkiINtyZqfa">http://wenku.baidu.com/link?url=G7mc7ywnFgAc62PAIVkSqVHbS3TRYrN0rI-q8tspmgnDFGRngeoKCM_FPL31s9K7G4RoA45fZzeFNyube_LzhZfrkjY8XO41MkiINtyZqfa</a>
<p>weka.classifiers.trees.j48
<p>机器学习十大算法（10）CART
<p>Classification and regression trees
<p>特点:采用二分递归分割技术，将当前样本分成两个子集，使得生成的每个非叶子节点都有两个分支。二叉树。
<p>针对离散属性进行分类，针对连续属性则进行回归。
<p>数据划分策略：Gini指标
<p><img title="clip_image003" style="border-top: 0px; border-right: 0px; background-image: none; border-bottom: 0px; padding-top: 0px; padding-left: 0px; border-left: 0px; display: inline; padding-right: 0px" border="0" alt="clip_image003" src="http://wp.hhy/wp-content/uploads/2014/10/clip_image003.gif" width="177" height="49">
<p>计算方式：
<p>对于每一个属性，划分成两部分，每部分数据中，计算Gini指标，然后用加权平均两部分的GinI指标作为该属性的Gini指标。权值为该部分数据所占的比例。
<p>在所有属性中选择属性Gini指标最小的一个作为决策节点，由此数据集被分成较纯的两部分。
<p>关键：根据属性值，划分成两部分，这个真子集比较难选，2^N种可能。
<p>有个问题：如果一个属性中的两个值的区别是区分某两个或两类样本的关键，在该属性的二元划分中没有将其划分开来，那怎么办？
<p>参考资料：
<p><a href="http://blog.csdn.net/hewei0241/article/details/8280490">http://blog.csdn.net/hewei0241/article/details/8280490</a>
<p><a href="http://blog.sina.com.cn/s/blog_7399ad1f01014oic.html">http://blog.sina.com.cn/s/blog_7399ad1f01014oic.html</a></p>
]]></content:encoded>
			<wfw:commentRss>http://wp.hhy/?feed=rss2&#038;p=15</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
		<item>
		<title>数据挖掘测试</title>
		<link>http://wp.hhy/?p=5</link>
		<comments>http://wp.hhy/?p=5#comments</comments>
		<pubDate>Fri, 03 Oct 2014 14:01:23 +0000</pubDate>
		<dc:creator><![CDATA[huahuiyou]]></dc:creator>
				<category><![CDATA[数据挖掘]]></category>

		<guid isPermaLink="false">http://wp.hhy/?p=5</guid>
		<description><![CDATA[测试数据挖掘]]></description>
				<content:encoded><![CDATA[<p>测试数据挖掘</p>
]]></content:encoded>
			<wfw:commentRss>http://wp.hhy/?feed=rss2&#038;p=5</wfw:commentRss>
		<slash:comments>0</slash:comments>
		</item>
	</channel>
</rss>
